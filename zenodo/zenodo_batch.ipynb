{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenodo Metadata Extraction for Digital Commons Batch Upload\n",
    "\n",
    "### What does this code do?\n",
    "\n",
    "This code searches Zenodo using the Zenodo API for datasets. There are two available search modes: searching by author affiliation or searching by Zenodo ID (number in the Zenodo URL). It then downloads those metadata records and converts them into a format that can be used for a Digital Commons bulk upload, with appropriate headers. This file will be saved as an .xlsx file and will require manual re-saving to a .xls file for the actual upload. \n",
    "\n",
    "### Additional Manual Curation\n",
    "\n",
    "This file will also require manual curation before it is ready for upload, with special attention to:\n",
    "\n",
    "- Cleaning up affiliations, ex:\n",
    "  - Unifying names for a single institution: University of Alabama - Birmingham, UNIVERSITY OF ALABAMA AT BIRMINGHAM\n",
    "   &rarr; University of Alabama at Birmingham\n",
    "  - Un-abbreviating institution names: NYU &rarr; New York University \n",
    "  - Removing extraneous location details: California Digital Library, Oakland, United States of America &rarr; California Digital Library\n",
    "- Checking for names and keywords written in all caps\n",
    "- Checking for special characters or accents that are not formatted properly\n",
    "\n",
    "### What datasets are included?\n",
    "\n",
    "We want to include find datasets where at least one author is affiliated with the University if Alabama at Birmingham. Since Zenodo does not use or require ROR IDs, the best way to find these datasets is by searching in the creator affiliation field for the query string \"university AND alabama AND birmingham\". Since [Zenodo hosts copies of Dryad datasets](https://blog.zenodo.org/2020/03/10/dryad-and-zenodo-our-path-ahead/), this search will locate datasets in both Zenodo and Dryad. As of 11/14/2024, there are 128 total datasets, 41 of which are from Dryad. The Dryad datasets belong to the Zenodo community \"dryad\", allowing us to isolate them if needed.\n",
    "\n",
    "### Import the data as a json\n",
    "\n",
    "The code below uses an access token (you will need to input your own or save it in a text file) and a search query ` \"creators.affiliation:(+university +alabama +birmingham)\" `. The `size` parameter allows you to choose the number of results returned. We have 128 results, so it is set at 200 to allow for some headroom if new datasets appear.\n",
    "\n",
    "We are left with `records`, the API response in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zenodo_access_token.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mzenodo_access_token.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file: \u001b[38;5;66;03m#save your access token in a txt file\u001b[39;00m\n\u001b[32m      5\u001b[39m     ACCESS_TOKEN = file.read().rstrip()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Define search queries for affiliation search and ID-specific search\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mihertz\\AppData\\Local\\miniconda3\\envs\\dataset-catalog\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'zenodo_access_token.txt'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "with open('zenodo_access_token.txt', 'r') as file: #save your access token in a txt file\n",
    "    ACCESS_TOKEN = file.read().rstrip()\n",
    "\n",
    "# Define search queries for affiliation search and ID-specific search\n",
    "search_query_affil = 'creators.affiliation:(+university +alabama +birmingham)'\n",
    "search_query_id = '14629136 + 14602463' # (include more as needed)\n",
    "\n",
    "# Select which query type you want to use, 'affiliation' or 'id'\n",
    "query_type_input = 'affiliation' # must be either 'affiliation' or 'id'\n",
    "\n",
    "def get_records(query_type):  \n",
    "        if query_type == 'affiliation':\n",
    "                search_query = search_query_affil\n",
    "        elif query_type == 'id':\n",
    "                search_query = search_query_id \n",
    "        response = requests.get('https://zenodo.org/api/records/',\n",
    "                        params={'q': search_query,\n",
    "                                'access_token': ACCESS_TOKEN,\n",
    "                                'size': 200, # should be ~128, add headroom\n",
    "                                'type' : 'dataset'\n",
    "                                })\n",
    "        records = response.json()\n",
    "        return records, query_type\n",
    "\n",
    "records, query_type = get_records(query_type_input)\n",
    "\n",
    "print(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the necessary dataframe\n",
    "\n",
    "The data from the API request comes in json format. We want to convert it to a pandas dataframe so that we can work with it more easily. This is done using the [`pd.json_normalize()`](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html) function. This results in a dataframe with a *lot* of columns. Some of these columns are irrelevant to our purposes (ex: view and download statistics) so we then remove them using the column names and the `drop()` function. \n",
    "\n",
    "#### Date Range for Affiliation Search\n",
    "\n",
    "You can input a start date by changing the `start_date` variable. This will remove the entries with a publication date earlier than your desired start date, so that you can avoid re-processing entries which you have already entered in your dataset catalog, and potentially manually curated.\n",
    "\n",
    "The end date (most recent record) defaults to the current date, but you can also set this manually using the `end_date` variable.\n",
    "\n",
    "These dates refer to the **creation date** of the dataset in the repository. You can change this (ex: if you want to look at the date when a dataset was most recently updated) by inputting a different column header.\n",
    "\n",
    "Note that this date range will only be affected if the query type is 'affiliation'.\n",
    "\n",
    "#### Saving\n",
    "\n",
    "We then save this dataframe in csv format. This allows us to easily view the metadata downloaded from Zenodo. We use the `utf-8-sig` encoding to best preserve symbols and special characters. The name format will be the following:\n",
    "\n",
    "For affiliation searches: zenodo_expanded_raw_YYYYMMDD_to_YYYYMMDD.csv, where the dates are the start and end dates set by you.\n",
    "For ID searches: zenodo_expanded_raw_ids_YYYYMMDD.csv, where the date is today's date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "os.makedirs('raw-data', exist_ok=True)\n",
    "os.makedirs('batch-upload', exist_ok=True)\n",
    "\n",
    "def records_to_dataframe(records, unwanted_cols, query_type):\n",
    "    ### MAKE THE DATAFRAME\n",
    "\n",
    "    df = pd.json_normalize(records['hits']['hits'][0])\n",
    "\n",
    "    for i in range(1, len(records['hits']['hits'])):\n",
    "        df_row = pd.json_normalize(records['hits']['hits'][i])\n",
    "\n",
    "        df = pd.concat([df, df_row])\n",
    "\n",
    "    ### DROP UNWANTED COLUMNS\n",
    "\n",
    "    df = df.drop(columns=unwanted_cols)\n",
    "\n",
    "    ### Remove records from before a certain date if this is an affiliation search\n",
    "\n",
    "    if query_type == 'affiliation':\n",
    "        df = df[df['created'] >= start_date]\n",
    "        df = df[df['created'] <= end_date]\n",
    "\n",
    "    ### Save to csv file, named according to query type\n",
    "\n",
    "    if query_type == 'affiliation':\n",
    "        date_range = start_date.replace('-', '') + '_to_' + end_date.replace('-', '')\n",
    "        csv_path = 'raw-data/zenodo_expanded_raw_' + date_range + '.csv'\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    elif query_type == 'id':\n",
    "        csv_path = 'raw-data/zenodo_expanded_raw_ids_' + today.replace('-', '') + '.csv'\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    return df\n",
    "\n",
    "unwanted_cols = ['conceptrecid', 'recid', 'revision', 'files', 'owners', 'status', 'state', 'submitted', 'metadata.title', 'metadata.resource_type.title', \n",
    "                 'metadata.resource_type.type', 'metadata.relations.version', 'links.self', 'links.doi', 'links.self_doi', 'links.self_doi_html', \n",
    "                 'links.parent', 'links.self_iiif_manifest', 'links.self_iiif_sequence', 'links.files', 'links.media_files', 'links.archive', 'links.archive_media', \n",
    "                 'links.latest', 'links.latest_html', 'links.versions', 'links.draft', 'links.reserve_doi', 'links.access_links', 'links.access_grants', 'links.access_users',\n",
    "                 'links.access_request', 'links.access', 'links.communities-suggestions', 'links.requests', 'stats.downloads', 'stats.unique_downloads',\n",
    "                 'stats.views', 'stats.unique_views', 'stats.version_downloads', 'stats.version_unique_downloads', 'stats.version_unique_views', 'stats.version_views'\n",
    "                ]\n",
    "\n",
    "today = str(datetime.date.today())\n",
    "start_date = '2025-04-25' # Set your start date here (YYYY-MM-DD)\n",
    "end_date = today # Change if you wish, 'YYYY-MM-DD' format\n",
    "\n",
    "df_input = records_to_dataframe(records, unwanted_cols, query_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We will need a series of functions to extract and reformat the information from the dataframe `df_input` and put it into new columns in the dataframe `df_output`, which will then be used to make the Digital Commons batch upload file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_dict(file_path):\n",
    "    '''Imports data from a 2-column csv file, where the first column contains dictionary keys and the second column contains the corresponding values.\n",
    "    Outputs the resulting dictionary. Used with the relation_types.csv file to generate the strings used for the relation type of a related item.'''\n",
    "    result_dict = {}\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            key = row[0]  # First column as key\n",
    "            value = row[1]  # Second column as value\n",
    "            result_dict[key] = value\n",
    "    return result_dict\n",
    "\n",
    "### Creating the relation type dictionary\n",
    "relation_dict = csv_to_dict('relation_types.csv')\n",
    "\n",
    "def add_col(df1, col1name, df2, col2name):\n",
    "    '''Copies a column (col1name) out of df1 and adds it to df2. The name of the column in df2 can be specified using the col2name variable.\n",
    "    The contents of the column are not altered. Returns df2 with the new column added.'''\n",
    "    extracted_col = df1[col1name]\n",
    "    df2 = pd.concat([df2, extracted_col.rename(col2name)], axis=1)\n",
    "    return df2\n",
    "\n",
    "def list_to_string(lst):\n",
    "    '''Takes in a list of strings ['a', 'b', 'c'] and returns a single string containing the list elements, separated by commas 'a, b, c'. '''\n",
    "    if isinstance(lst, list):  # Check if the value is a list\n",
    "        return ', '.join(lst)\n",
    "    else:\n",
    "        return \"\"  # Convert non-list values to string or handle as needed\n",
    "\n",
    "def url_to_html(url, link_text=None):\n",
    "    '''Converts a url in string form to a html formatted string for a hyperlinked url, with an optional alternate link text.'''\n",
    "    if not url:\n",
    "        return ''  # Return empty string if no URL is provided\n",
    "    link_text = link_text or url  # Use the URL as the link text if no text is provided\n",
    "    return f'<a href=\"{url}\">{link_text}</a>'\n",
    "\n",
    "### Dictionary containing the Zenodo terms for licenses as keys, with the values being a list containing the license URL as well as the html formatted text corresponding to each license.\n",
    "license_dict = {\"mit-license\" : [\"https://opensource.org/license/mit\", \"<p>This data is available under the MIT License</p>\"],\n",
    "                \"cc-zero\" : [\"https://creativecommons.org/public-domain/cc0/\", \"<p>This data is public domain under the CC-0.0 License</p>\"],\n",
    "                \"cc-by-4.0\" : [\"http://creativecommons.org/licenses/by/4.0/\", \"<p>This data is available under the CC-BY 4.0 License</p>\"],\n",
    "                \"cc-by-nc-4.0\" : [\"http://creativecommons.org/licenses/by-nc/4.0/\", \"<p>This data is available under the CC-BY-NC 4.0 License</p>\"],\n",
    "                \"cc-by-2.0\" : [\"http://creativecommons.org/licenses/by/2.0/\", \"<p>This data is available under the CC-BY 2.0 License</p>\"],\n",
    "                \"cc-by\" : [\"https://creativecommons.org/licenses/by/1.0/\", \"<p>This data is available under the CC-BY License</p>\"]\n",
    "}\n",
    "\n",
    "def add_license(df1, df2):\n",
    "    '''Adds licensing information columns to df2. Finds the value of the column metadata.license.id in df1\n",
    "    and uses it as a key for license_dict to retrieve the url fo the license and the string we want displayed in html.\n",
    "    If there is no value in that row, there is no license shown. We assume this means the data is restricted.'''\n",
    "    licenses = [] # List for the license url\n",
    "    access = [] # List for the string/text explaining access\n",
    "    for license in df1['metadata.license.id']:\n",
    "        if pd.notnull(license):\n",
    "            licenses.append(license_dict[license][0])\n",
    "            access.append(license_dict[license][1])\n",
    "        else:\n",
    "            licenses.append('') # Append blank string if there is no license for restricted data\n",
    "            access.append('<p>Access to this data is restricted.</p>')\n",
    "    df2['distribution_license'] = licenses\n",
    "    df2['access_link'] = access\n",
    "    return df2\n",
    "\n",
    "def add_repo(df1, df2):\n",
    "    '''Adds the repository in which the data is stored. Defaults to Zenodo, unless the dataset is in the Dryad community.'''\n",
    "    repo_list = []\n",
    "    for community in df1['metadata.communities']: # Iterates through metadata.communities in df1\n",
    "        if pd.notnull(community) and community[0]['id'] == 'dryad': # If there is a community listed and it is Dryad\n",
    "            repo_list.append('<p>Dryad</p>')\n",
    "        else:\n",
    "            repo_list.append('<p>Zenodo</p>')\n",
    "    df2['external_rep'] = repo_list\n",
    "    return df2\n",
    "\n",
    "def separate_name(name):\n",
    "    '''Separates a name into First, Middle (if applicable) and Last, returns these elements as separate strings.'''\n",
    "    middle = \"\"\n",
    "    \n",
    "    # Check if the name contains a comma (indicating \"last, first\" or \"last, first m.\" format)\n",
    "    if ',' in name:\n",
    "        parts = name.split(\", \") # Split into a list: \"last, first\" becomes [\"last\", \"first\"]\n",
    "        last = parts[0] # Last name is everything before the comma\n",
    "        \n",
    "        # Check if there's an element after the comma\n",
    "        if len(parts) > 1:\n",
    "            first_and_middle = parts[1].split() # Split whatever was after the comma with spaces\n",
    "            first = first_and_middle[0] # First name will be the first part of that\n",
    "            \n",
    "            # Assign middle if available\n",
    "            if len(first_and_middle) > 1: # If there is a second part\n",
    "                middle = first_and_middle[1].replace(\".\", \"\") # Assign middle name as second part, remove . because DC will add it\n",
    "        else:\n",
    "            # Set first to an empty string if there's nothing after the comma\n",
    "            first = \"\"\n",
    "    else: # If the name had no comma, you can assume it is in First Last or First Middle Last format\n",
    "        parts = name.split() # Split into a list where spaces are \n",
    "        \n",
    "        first = parts[0] # First name is first element of list\n",
    "        last = parts[-1] # Last name is the last element of the list \n",
    "        \n",
    "        # Check if there is a middle name/initial \n",
    "        if len(parts) > 2: \n",
    "            middle = parts[1].replace(\".\", \"\") # Remove period if it exists\n",
    "    \n",
    "    return first, middle, last\n",
    "    \n",
    "def reformat_name(name):\n",
    "    '''Checks if a name string is in \"Last, First\" format. If it is, returns \"First Last\".'''\n",
    "    # Check if the name contains a comma\n",
    "    if ', ' in name:\n",
    "        # Split the string by comma and strip any extra whitespace\n",
    "        last, first = name.split(\", \")\n",
    "        # Return the string in \"first last\" format\n",
    "        return f\"{first} {last}\"\n",
    "    else:\n",
    "        # Return the name unchanged if there's no comma\n",
    "        return name\n",
    "    \n",
    "def add_orcid(df1, df2):\n",
    "    '''Finds authors with ORCIDs and lists them in html format for each dataset, along with hyperlinked urls.'''\n",
    "    orcid_pairs = [] # List for lists of author/orcid pairs where each element corresponds to a different dataset \n",
    "    for index, row in df1.iterrows():\n",
    "        pairs = [] # List for appending author/orcid pairs within one dataset\n",
    "        for author in row['metadata.creators']: # Iterate through the list of authors in each row of the metadata.creators column\n",
    "            if 'orcid' in author:  # Check if ORCID is present\n",
    "                first_last_name = reformat_name(author['name'])\n",
    "                pairs.append('<p>' + first_last_name + ' <a href=\"https://orcid.org/' + author['orcid'] + '\">' +author['orcid']+ '</a></p>')\n",
    "        orcid_pairs.append(\"\".join(pairs))  # Join multiple name-ORCID pairs with a comma\n",
    "    # Add this list as a new column in df2\n",
    "    df2['orcid'] = orcid_pairs\n",
    "    return df2\n",
    "\n",
    "def to_html(string_input):\n",
    "    string = str(string_input)\n",
    "    '''Takes in a string. If the string is not in html already (assume first character is <) wrap it in <p> ... </p>.'''\n",
    "    if string[0] != \"<\":\n",
    "        string = \"<p>\" + string + \"</p>\"\n",
    "    return(string)\n",
    "\n",
    "def add_funders(df1, df2):\n",
    "    '''Adds funder information from df1 to df2. Adds funder name, and optionally DOI, grant title, and grant number.'''\n",
    "    funders = []\n",
    "    for index, row in df1.iterrows():\n",
    "        funder = []\n",
    "        # Iterate through the grants if they are present\n",
    "        if isinstance(row['metadata.grants'], list):\n",
    "            for grant in row['metadata.grants']:\n",
    "                if pd.notnull(grant):  # Check if grant is not null\n",
    "                    funder.append('<p>Funder: ' + grant['funder']['name'])\n",
    "                    if 'doi' in grant['funder']:\n",
    "                        funder.append('<br>Funder DOI: <a href=\"https://doi.org/' + grant['funder']['doi'] + '\">' +grant['funder']['doi']+ '</a>')\n",
    "                    if 'title' in grant:\n",
    "                        funder.append('<br>' + grant['title'])\n",
    "                    if 'code' in grant:\n",
    "                        funder.append('<br>' + grant['code'])\n",
    "                    funder.append('</p>')\n",
    "        else:\n",
    "            # If it's not a list put an empty cell\n",
    "            if pd.notnull(row['metadata.grants']):\n",
    "                funder.append('')\n",
    "        # Append the joined funder information to the list\n",
    "        funders.append(\"\".join(funder))\n",
    "    # Add the new 'fundref' column to df2\n",
    "    df2['fundref'] = funders\n",
    "    return df2\n",
    "\n",
    "def add_related_items(df1, df2):\n",
    "    '''Adds related items from df1 to df2 with nice html formatting. Includes relation type taken from relation_dict dictionary. Formats PID appropriately if it is a DOI or other URL.'''\n",
    "    items = []\n",
    "    for index, row in df1.iterrows():\n",
    "        item = []\n",
    "        # Iterate through the related items if they are present\n",
    "        if isinstance(row['metadata.related_identifiers'], list):\n",
    "            item.append('<p>')\n",
    "            count =0\n",
    "            for id in row['metadata.related_identifiers']:\n",
    "                if pd.notnull(id):  \n",
    "                    if count > 0:\n",
    "                        item.append('<br>')\n",
    "                    count += 1\n",
    "                    if id['relation']:\n",
    "                        item.append(relation_dict[id['relation']] + ': ')\n",
    "                    if id['scheme'] == 'url':\n",
    "                        url = id['identifier']\n",
    "                        item.append( '<a href=\"'+ url + '\">' + url + '</a>')\n",
    "                    if id['scheme'] == 'doi':\n",
    "                        doi = id['identifier']\n",
    "                        item.append('<a href=\"https://doi.org/' + doi + '\">' + doi + '</a>')\n",
    "                    if id['scheme'] != 'url' and id['scheme'] != 'doi':\n",
    "                        item.append(id['identifier'])\n",
    "            item.append('</p>')\n",
    "        else:\n",
    "            # If it's not a list put an empty cell\n",
    "            if pd.notnull(row['metadata.related_identifiers']):\n",
    "                item.append('')\n",
    "        # Append the joined funder information to the list\n",
    "        items.append(\"\".join(item))\n",
    "    # Add the new 'fundref' column to df2\n",
    "    df2['related_data'] = items\n",
    "    return df2\n",
    "\n",
    "def add_creators(df1, df2):\n",
    "    '''Adds creator information. Note that this adds an arbitrary number of creators but Digital Commons has a number cap for authors so you may need to manually curate after.\n",
    "    This function will make a new dataframe df3 and append it on to df2.'''\n",
    "    # Create a list to hold all rows of data for the new DataFrame\n",
    "    expanded_data = []\n",
    "    \n",
    "    # Process each row in df1\n",
    "    for _, row in df1.iterrows():\n",
    "        row_data = {}\n",
    "        creators = row['metadata.creators']\n",
    "        \n",
    "        # Populate the row_data dictionary with each author's name and affiliation\n",
    "        for i, creator in enumerate(creators):\n",
    "            author_index = i + 1\n",
    "            name = creator.get('name', \"\")\n",
    "            institution = creator.get('affiliation', \"\")  # Use 'institution' instead of 'affiliation' (DC nomenclature)\n",
    "            \n",
    "            # Use the separate_name function to split names\n",
    "            first_name, middle_name, last_name = separate_name(name)\n",
    "            \n",
    "            # Assign names and institution to the row_data dictionary\n",
    "            row_data[f'author{author_index}_fname'] = first_name\n",
    "            row_data[f'author{author_index}_mname'] = middle_name\n",
    "            row_data[f'author{author_index}_lname'] = last_name\n",
    "            row_data[f'author{author_index}_institution'] = institution  # Change 'affl' to 'institution'\n",
    "        \n",
    "        # Append row_data dictionary to expanded_data list\n",
    "        expanded_data.append(row_data)\n",
    "\n",
    "    # Convert expanded_data list of dictionaries into a new DataFrame\n",
    "    df3 = pd.DataFrame(expanded_data)\n",
    "    \n",
    "    # Fill missing values with empty strings for any columns where data is missing\n",
    "    df3 = df3.fillna(\"\")\n",
    "    \n",
    "    df2_reset = df2.reset_index(drop=True)\n",
    "    df3_reset = df3.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the two DataFrames along the columns\n",
    "    df_out = pd.concat([df2_reset, df3_reset], axis=1)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build output dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_27936\\1865036149.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['orcid'] = orcid_pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>orcid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>disciplines</th>\n",
       "      <th>source_publication</th>\n",
       "      <th>related_data</th>\n",
       "      <th>source_fulltext_url</th>\n",
       "      <th>external_rep</th>\n",
       "      <th>...</th>\n",
       "      <th>author72_lname</th>\n",
       "      <th>author72_institution</th>\n",
       "      <th>author73_fname</th>\n",
       "      <th>author73_mname</th>\n",
       "      <th>author73_lname</th>\n",
       "      <th>author73_institution</th>\n",
       "      <th>author74_fname</th>\n",
       "      <th>author74_mname</th>\n",
       "      <th>author74_lname</th>\n",
       "      <th>author74_institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chromatin Structural Gene Expression Stratifie...</td>\n",
       "      <td>&lt;p&gt;Manuel Rosa-Garrido &lt;a href=\"https://orcid....</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>&lt;p&gt;Chromatin structure plays a central role in...</td>\n",
       "      <td>HMGN3, Cardiac Disease, Chromatin Structure</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.17035888</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data related to \"Tau, amyloid-beta and alpha-s...</td>\n",
       "      <td>&lt;p&gt;Jhodi Webster &lt;a href=\"https://orcid.org/00...</td>\n",
       "      <td>2025-08-22</td>\n",
       "      <td>&lt;h3&gt;&lt;strong&gt;Manuscript Abstract&lt;/strong&gt;&lt;/h3&gt;\\...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.16739908</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICTV Master Species List 2024 MSL40.v2</td>\n",
       "      <td>&lt;p&gt;Elliot Lefkowitz &lt;a href=\"https://orcid.org...</td>\n",
       "      <td>2025-08-26</td>\n",
       "      <td>&lt;p&gt;The Master Species List (MSL) provides acce...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.16955220</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accelerating Drug Repurposing for Rett Syndrom...</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>&lt;p&gt;Above are the datasets that were pulled fro...</td>\n",
       "      <td>Drug Repurposing, INFO603</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is supplemented by: PharmAlchemy&lt;/p&gt;</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15278272</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Single Cell RNASeq of motor cortex in a mouse ...</td>\n",
       "      <td>&lt;p&gt;Viktor Feketa &lt;a href=\"https://orcid.org/00...</td>\n",
       "      <td>2025-06-12</td>\n",
       "      <td>&lt;p&gt;This Zenodo deposit contains a publicly ava...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15649798</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Striatal dopamine signals errors in prediction...</td>\n",
       "      <td>&lt;p&gt;Kauê Costa &lt;a href=\"https://orcid.org/0000-...</td>\n",
       "      <td>2025-02-03</td>\n",
       "      <td>&lt;p&gt;nan&lt;/p&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.14792481</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Disrupted development of sensory systems and t...</td>\n",
       "      <td>&lt;p&gt;Summer Thyme &lt;a href=\"https://orcid.org/000...</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>&lt;p&gt;Behavior and imaging data for the ebf3a mut...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15304810</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Disrupted Diencephalon Development and Neurope...</td>\n",
       "      <td>&lt;p&gt;Summer Thyme &lt;a href=\"https://orcid.org/000...</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>&lt;p&gt;Imaging, RNA-seq, and behavioral data to su...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15318760</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Monoicy, dioicy, and genetic structure in thre...</td>\n",
       "      <td>&lt;p&gt;Sarah Shainker-Connelly &lt;a href=\"https://or...</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>&lt;p&gt;All multilocus haploid genotypes are provid...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15381199</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A CLONAL LEGACY? REPRODUCTIVE MODE VARIATION I...</td>\n",
       "      <td>&lt;p&gt;Alexis Oetterer &lt;a href=\"https://orcid.org/...</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;The &lt;/span&gt;red macroalga &lt;em&gt;&lt;span&gt;Gr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15738886</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Type 2 diabetes polygenic risk score demonstra...</td>\n",
       "      <td>&lt;p&gt;Boya Guo &lt;a href=\"https://orcid.org/0000-00...</td>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>&lt;p&gt;Polygenic risk scores (PRS) hold prognostic...</td>\n",
       "      <td>PheWAS, Polygenic risk score</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is published in: &lt;a href=\"https://doi.org/1...</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15998801</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>Raghavan</td>\n",
       "      <td>University of Colorado Cancer Center</td>\n",
       "      <td>Heather</td>\n",
       "      <td>M</td>\n",
       "      <td>Highland</td>\n",
       "      <td>University of North Carolina at Chapel Hill</td>\n",
       "      <td>Burcu</td>\n",
       "      <td>F</td>\n",
       "      <td>Darst</td>\n",
       "      <td>Fred Hutch Cancer Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PharmaKG Explorer: Knowledge Graph Embedding-D...</td>\n",
       "      <td>&lt;p&gt;Shehan Irteza Pranto &lt;a href=\"https://orcid...</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>&lt;p&gt;In this project, we develop an end-to-end p...</td>\n",
       "      <td>Knowledge Graph Embedding, Neo4j Aura, Link Pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15330597</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dataset Medical Noise Image (FFTMed)</td>\n",
       "      <td>&lt;p&gt;Viet Tien Pham &lt;a href=\"https://orcid.org/0...</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>&lt;p&gt;This dataset here public available&lt;/p&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Has version: &lt;a href=\"https://www.biorxiv.o...</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.15310397</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Chromatin Structural Gene Expression Stratifie...   \n",
       "1   Data related to \"Tau, amyloid-beta and alpha-s...   \n",
       "2              ICTV Master Species List 2024 MSL40.v2   \n",
       "3   Accelerating Drug Repurposing for Rett Syndrom...   \n",
       "4   Single Cell RNASeq of motor cortex in a mouse ...   \n",
       "5   Striatal dopamine signals errors in prediction...   \n",
       "6   Disrupted development of sensory systems and t...   \n",
       "7   Disrupted Diencephalon Development and Neurope...   \n",
       "8   Monoicy, dioicy, and genetic structure in thre...   \n",
       "9   A CLONAL LEGACY? REPRODUCTIVE MODE VARIATION I...   \n",
       "10  Type 2 diabetes polygenic risk score demonstra...   \n",
       "11  PharmaKG Explorer: Knowledge Graph Embedding-D...   \n",
       "12               Dataset Medical Noise Image (FFTMed)   \n",
       "\n",
       "                                                orcid publication_date  \\\n",
       "0   <p>Manuel Rosa-Garrido <a href=\"https://orcid....       2025-09-02   \n",
       "1   <p>Jhodi Webster <a href=\"https://orcid.org/00...       2025-08-22   \n",
       "2   <p>Elliot Lefkowitz <a href=\"https://orcid.org...       2025-08-26   \n",
       "3                                                           2025-04-25   \n",
       "4   <p>Viktor Feketa <a href=\"https://orcid.org/00...       2025-06-12   \n",
       "5   <p>Kauê Costa <a href=\"https://orcid.org/0000-...       2025-02-03   \n",
       "6   <p>Summer Thyme <a href=\"https://orcid.org/000...       2025-04-29   \n",
       "7   <p>Summer Thyme <a href=\"https://orcid.org/000...       2025-05-01   \n",
       "8   <p>Sarah Shainker-Connelly <a href=\"https://or...       2025-05-09   \n",
       "9   <p>Alexis Oetterer <a href=\"https://orcid.org/...       2025-06-25   \n",
       "10  <p>Boya Guo <a href=\"https://orcid.org/0000-00...       2025-07-16   \n",
       "11  <p>Shehan Irteza Pranto <a href=\"https://orcid...       2025-05-02   \n",
       "12  <p>Viet Tien Pham <a href=\"https://orcid.org/0...       2025-04-30   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   <p>Chromatin structure plays a central role in...   \n",
       "1   <h3><strong>Manuscript Abstract</strong></h3>\\...   \n",
       "2   <p>The Master Species List (MSL) provides acce...   \n",
       "3   <p>Above are the datasets that were pulled fro...   \n",
       "4   <p>This Zenodo deposit contains a publicly ava...   \n",
       "5                                          <p>nan</p>   \n",
       "6   <p>Behavior and imaging data for the ebf3a mut...   \n",
       "7   <p>Imaging, RNA-seq, and behavioral data to su...   \n",
       "8   <p>All multilocus haploid genotypes are provid...   \n",
       "9   <p><span>The </span>red macroalga <em><span>Gr...   \n",
       "10  <p>Polygenic risk scores (PRS) hold prognostic...   \n",
       "11  <p>In this project, we develop an end-to-end p...   \n",
       "12          <p>This dataset here public available</p>   \n",
       "\n",
       "                                             keywords disciplines  \\\n",
       "0         HMGN3, Cardiac Disease, Chromatin Structure               \n",
       "1                                                                   \n",
       "2                                                                   \n",
       "3                           Drug Repurposing, INFO603               \n",
       "4                                                                   \n",
       "5                                                                   \n",
       "6                                                                   \n",
       "7                                                                   \n",
       "8                                                                   \n",
       "9                                                                   \n",
       "10                       PheWAS, Polygenic risk score               \n",
       "11  Knowledge Graph Embedding, Neo4j Aura, Link Pr...               \n",
       "12                                                                  \n",
       "\n",
       "   source_publication                                       related_data  \\\n",
       "0                                                                          \n",
       "1                                                                          \n",
       "2                                                                          \n",
       "3                                <p>Is supplemented by: PharmAlchemy</p>   \n",
       "4                                                                          \n",
       "5                                                                          \n",
       "6                                                                          \n",
       "7                                                                          \n",
       "8                                                                          \n",
       "9                                                                          \n",
       "10                     <p>Is published in: <a href=\"https://doi.org/1...   \n",
       "11                                                                         \n",
       "12                     <p>Has version: <a href=\"https://www.biorxiv.o...   \n",
       "\n",
       "                        source_fulltext_url   external_rep  ...  \\\n",
       "0   https://doi.org/10.5281/zenodo.17035888  <p>Zenodo</p>  ...   \n",
       "1   https://doi.org/10.5281/zenodo.16739908  <p>Zenodo</p>  ...   \n",
       "2   https://doi.org/10.5281/zenodo.16955220  <p>Zenodo</p>  ...   \n",
       "3   https://doi.org/10.5281/zenodo.15278272  <p>Zenodo</p>  ...   \n",
       "4   https://doi.org/10.5281/zenodo.15649798  <p>Zenodo</p>  ...   \n",
       "5   https://doi.org/10.5281/zenodo.14792481  <p>Zenodo</p>  ...   \n",
       "6   https://doi.org/10.5281/zenodo.15304810  <p>Zenodo</p>  ...   \n",
       "7   https://doi.org/10.5281/zenodo.15318760  <p>Zenodo</p>  ...   \n",
       "8   https://doi.org/10.5281/zenodo.15381199  <p>Zenodo</p>  ...   \n",
       "9   https://doi.org/10.5281/zenodo.15738886  <p>Zenodo</p>  ...   \n",
       "10  https://doi.org/10.5281/zenodo.15998801  <p>Zenodo</p>  ...   \n",
       "11  https://doi.org/10.5281/zenodo.15330597  <p>Zenodo</p>  ...   \n",
       "12  https://doi.org/10.5281/zenodo.15310397  <p>Zenodo</p>  ...   \n",
       "\n",
       "   author72_lname                  author72_institution author73_fname  \\\n",
       "0                                                                        \n",
       "1                                                                        \n",
       "2                                                                        \n",
       "3                                                                        \n",
       "4                                                                        \n",
       "5                                                                        \n",
       "6                                                                        \n",
       "7                                                                        \n",
       "8                                                                        \n",
       "9                                                                        \n",
       "10       Raghavan  University of Colorado Cancer Center        Heather   \n",
       "11                                                                       \n",
       "12                                                                       \n",
       "\n",
       "   author73_mname author73_lname                         author73_institution  \\\n",
       "0                                                                               \n",
       "1                                                                               \n",
       "2                                                                               \n",
       "3                                                                               \n",
       "4                                                                               \n",
       "5                                                                               \n",
       "6                                                                               \n",
       "7                                                                               \n",
       "8                                                                               \n",
       "9                                                                               \n",
       "10              M       Highland  University of North Carolina at Chapel Hill   \n",
       "11                                                                              \n",
       "12                                                                              \n",
       "\n",
       "   author74_fname author74_mname author74_lname      author74_institution  \n",
       "0                                                                          \n",
       "1                                                                          \n",
       "2                                                                          \n",
       "3                                                                          \n",
       "4                                                                          \n",
       "5                                                                          \n",
       "6                                                                          \n",
       "7                                                                          \n",
       "8                                                                          \n",
       "9                                                                          \n",
       "10          Burcu              F          Darst  Fred Hutch Cancer Center  \n",
       "11                                                                         \n",
       "12                                                                         \n",
       "\n",
       "[13 rows x 309 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### BUILD OUTPUT DATAFRAME\n",
    "\n",
    "# title\n",
    "df_output = df_input[['title']]\n",
    "\n",
    "# orcid\n",
    "df_output = add_orcid(df1=df_input, df2=df_output)\n",
    "\n",
    "# publication_date\n",
    "df_output = add_col(df_input, \"metadata.publication_date\", df_output, \"publication_date\")\n",
    "\n",
    "# abstract\n",
    "df_output = add_col(df_input, \"metadata.description\", df_output, \"abstract\")\n",
    "df_output['abstract'] = df_output['abstract'].apply(to_html)\n",
    "\n",
    "# keywords\n",
    "df_output = add_col(df_input, \"metadata.keywords\", df_output, \"keywords\")\n",
    "df_output[\"keywords\"] = df_output[\"keywords\"].apply(list_to_string)\n",
    "\n",
    "# disciplines\n",
    "df_output[\"disciplines\"] = \"\" #make blank column, we will need to fill in the values\n",
    "\n",
    "#source_publication\n",
    "df_output[\"source_publication\"] = \"\" #make blank column, we will need to fill in the values\n",
    "\n",
    "# related_data\n",
    "df_output = add_related_items(df1=df_input, df2=df_output)\n",
    "\n",
    "# source_fulltext_url\n",
    "df_output = add_col(df_input, \"doi_url\", df_output, \"source_fulltext_url\")\n",
    "\n",
    "# external_rep\n",
    "df_output = add_repo(df1=df_input, df2=df_output)\n",
    "\n",
    "# distribution_license and access_link\n",
    "df_output = add_license(df1=df_input, df2=df_output)\n",
    "\n",
    "# funder_info\n",
    "df_output = add_funders(df1=df_input, df2=df_output)\n",
    "\n",
    "# author info\n",
    "df_output = add_creators(df1=df_input, df2=df_output)\n",
    "\n",
    "display(df_output)\n",
    "\n",
    "df_output.to_excel('batch-upload/zenodo_batch_upload_' + today + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-catalog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
