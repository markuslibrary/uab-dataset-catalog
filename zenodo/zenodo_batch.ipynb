{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenodo Metadata Extraction for Digital Commons Batch Upload\n",
    "\n",
    "### What does this code do?\n",
    "\n",
    "This code searches Zenodo using the Zenodo API for datasets. There are two available search modes: searching by author affiliation or searching by Zenodo ID (number in the Zenodo URL). It then downloads those metadata records and converts them into a format that can be used for a Digital Commons bulk upload, with appropriate headers. This file will be saved as an .xlsx file and will require manual re-saving to a .xls file for the actual upload. \n",
    "\n",
    "### Additional Manual Curation\n",
    "\n",
    "This file will also require manual curation before it is ready for upload, with special attention to:\n",
    "\n",
    "- Cleaning up affiliations, ex:\n",
    "  - Unifying names for a single institution: University of Alabama - Birmingham, UNIVERSITY OF ALABAMA AT BIRMINGHAM\n",
    "   &rarr; University of Alabama at Birmingham\n",
    "  - Un-abbreviating institution names: NYU &rarr; New York University \n",
    "  - Removing extraneous location details: California Digital Library, Oakland, United States of America &rarr; California Digital Library\n",
    "- Checking for names and keywords written in all caps\n",
    "- Checking for special characters or accents that are not formatted properly\n",
    "\n",
    "### What datasets are included?\n",
    "\n",
    "We want to include find datasets where at least one author is affiliated with the University if Alabama at Birmingham. Since Zenodo does not use or require ROR IDs, the best way to find these datasets is by searching in the creator affiliation field for the query string \"university AND alabama AND birmingham\". Since [Zenodo hosts copies of Dryad datasets](https://blog.zenodo.org/2020/03/10/dryad-and-zenodo-our-path-ahead/), this search will locate datasets in both Zenodo and Dryad. As of 11/14/2024, there are 128 total datasets, 41 of which are from Dryad. The Dryad datasets belong to the Zenodo community \"dryad\", allowing us to isolate them if needed.\n",
    "\n",
    "### Import the data as a json\n",
    "\n",
    "The code below uses an access token (you will need to input your own or save it in a text file) and a search query ` \"creators.affiliation:(+university +alabama +birmingham)\" `. The `size` parameter allows you to choose the number of results returned. We have 128 results, so it is set at 200 to allow for some headroom if new datasets appear.\n",
    "\n",
    "We are left with `records`, the API response in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hits': {'hits': [{'created': '2025-01-10T21:33:25.266228+00:00', 'modified': '2025-01-10T21:33:25.480762+00:00', 'id': 14602463, 'conceptrecid': '14602462', 'doi': '10.5281/zenodo.14602463', 'conceptdoi': '10.5281/zenodo.14602462', 'doi_url': 'https://doi.org/10.5281/zenodo.14602463', 'metadata': {'title': 'Consumers Modulate Effects of Plant Diversity on Community Stability', 'doi': '10.5281/zenodo.14602463', 'publication_date': '2025-01-10', 'description': '<p>All data and code associated with the publications: Liang et al. (2025) Consumers Modulate Effects of Plant Diversity on Community Stability.</p>\\n<p>Original datasets are available at the Environmental Data Initiative (https://doi.org/10.6073/pasta/d35065bdbebf73b310a21815616f66b2).&nbsp;</p>', 'access_right': 'open', 'creators': [{'name': 'Liang, Maowei', 'affiliation': None, 'orcid': '0000-0002-1517-0497'}, {'name': 'Cappelli, Seraina Lisa', 'affiliation': None, 'orcid': '0000-0002-8141-404X'}, {'name': 'Borer, Elizabeth', 'affiliation': 'University of Minnesota Twin Cities', 'orcid': '0000-0003-2259-5853'}, {'name': 'Tilman, David', 'affiliation': 'University of Minnesota Twin Cities', 'orcid': '0000-0001-6395-7676'}, {'name': 'Seabloom, Eric', 'affiliation': 'University of Minnesota System', 'orcid': '0000-0001-6780-9259'}], 'keywords': ['Asynchrony', 'Insurance effects', 'Trophic interactions'], 'version': '1.0', 'custom': {'code:programmingLanguage': [{'id': 'r', 'title': {'en': 'R'}}], 'code:developmentStatus': {'id': 'active', 'title': {'en': 'Active'}}}, 'resource_type': {'title': 'Dataset', 'type': 'dataset'}, 'license': {'id': 'cc-by-4.0'}, 'grants': [{'code': '1831944', 'internal_id': '10.13039/100000001::1831944', 'funder': {'name': 'U.S. National Science Foundation', 'doi': '10.13039/100000001', 'acronym': 'NSF'}, 'title': 'LTER:  Multi-decadal responses of prairie, savanna, and forest ecosystems to interacting environmental changes: insights from experiments, observations, and models', 'program': 'BIO/OAD'}], 'relations': {'version': [{'index': 0, 'is_last': True, 'parent': {'pid_type': 'recid', 'pid_value': '14602462'}}]}}, 'title': 'Consumers Modulate Effects of Plant Diversity on Community Stability', 'links': {'self': 'https://zenodo.org/api/records/14602463', 'self_html': 'https://zenodo.org/records/14602463', 'preview_html': 'https://zenodo.org/records/14602463?preview=1', 'doi': 'https://doi.org/10.5281/zenodo.14602463', 'self_doi': 'https://doi.org/10.5281/zenodo.14602463', 'self_doi_html': 'https://zenodo.org/doi/10.5281/zenodo.14602463', 'parent': 'https://zenodo.org/api/records/14602462', 'parent_html': 'https://zenodo.org/records/14602462', 'parent_doi': 'https://doi.org/10.5281/zenodo.14602462', 'parent_doi_html': 'https://zenodo.org/doi/10.5281/zenodo.14602462', 'self_iiif_manifest': 'https://zenodo.org/api/iiif/record:14602463/manifest', 'self_iiif_sequence': 'https://zenodo.org/api/iiif/record:14602463/sequence/default', 'files': 'https://zenodo.org/api/records/14602463/files', 'media_files': 'https://zenodo.org/api/records/14602463/media-files', 'archive': 'https://zenodo.org/api/records/14602463/files-archive', 'archive_media': 'https://zenodo.org/api/records/14602463/media-files-archive', 'latest': 'https://zenodo.org/api/records/14602463/versions/latest', 'latest_html': 'https://zenodo.org/records/14602463/latest', 'versions': 'https://zenodo.org/api/records/14602463/versions', 'draft': 'https://zenodo.org/api/records/14602463/draft', 'reserve_doi': 'https://zenodo.org/api/records/14602463/draft/pids/doi', 'access_links': 'https://zenodo.org/api/records/14602463/access/links', 'access_grants': 'https://zenodo.org/api/records/14602463/access/grants', 'access_users': 'https://zenodo.org/api/records/14602463/access/users', 'access_request': 'https://zenodo.org/api/records/14602463/access/request', 'access': 'https://zenodo.org/api/records/14602463/access', 'communities': 'https://zenodo.org/api/records/14602463/communities', 'communities-suggestions': 'https://zenodo.org/api/records/14602463/communities-suggestions', 'requests': 'https://zenodo.org/api/records/14602463/requests'}, 'updated': '2025-01-10T21:33:25.480762+00:00', 'recid': '14602463', 'revision': 4, 'files': [{'id': '40f7b55f-bbd9-4d7e-8d17-7122b454d6a8', 'key': 'Metadata.xlsx', 'size': 11497, 'checksum': 'md5:97e0248f1bd3b78b09ca705602a76d65', 'links': {'self': 'https://zenodo.org/api/records/14602463/files/Metadata.xlsx/content'}}, {'id': '72da1093-98ba-46c5-b1d7-20eaa46ad52d', 'key': 'Sensitivity_analysis.R', 'size': 24813, 'checksum': 'md5:209273e3509c47bea7f89d9dee294309', 'links': {'self': 'https://zenodo.org/api/records/14602463/files/Sensitivity_analysis.R/content'}}, {'id': '800d31d8-be33-4245-bf86-1adc288d295b', 'key': 'Consumers_div_sta.R', 'size': 47542, 'checksum': 'md5:b61d773fe2c99e9415ba99a7e3eb3a3f', 'links': {'self': 'https://zenodo.org/api/records/14602463/files/Consumers_div_sta.R/content'}}, {'id': '09e81a95-79d8-4047-aee4-19bc67498a0a', 'key': 'Consumers_div_sta.csv', 'size': 66895, 'checksum': 'md5:e068cd71228363acc8374b563ad6c804', 'links': {'self': 'https://zenodo.org/api/records/14602463/files/Consumers_div_sta.csv/content'}}, {'id': '91e2421b-becb-49a4-96d8-d47172bcec0b', 'key': 'Consumers_div_pro.csv', 'size': 183483, 'checksum': 'md5:09d7874b280622cee30c395e4500104f', 'links': {'self': 'https://zenodo.org/api/records/14602463/files/Consumers_div_pro.csv/content'}}], 'swh': {}, 'owners': [{'id': '1202950'}], 'status': 'published', 'stats': {'downloads': 152, 'unique_downloads': 128, 'views': 60, 'unique_views': 52, 'version_downloads': 152, 'version_unique_downloads': 128, 'version_unique_views': 52, 'version_views': 60}, 'state': 'done', 'submitted': True}, {'created': '2025-01-10T16:24:25.356837+00:00', 'modified': '2025-01-10T16:24:42.360432+00:00', 'id': 14629136, 'conceptrecid': '14531351', 'doi': '10.5281/zenodo.14629136', 'conceptdoi': '10.5281/zenodo.14531351', 'doi_url': 'https://doi.org/10.5281/zenodo.14629136', 'metadata': {'title': 'FractiScope Spiral Galaxy Cluster SMACS 0723 Expedition', 'doi': '10.5281/zenodo.14629136', 'publication_date': '2025-01-10', 'description': \"<p><strong>Welcome to the FractiScope Spiral Galaxy Cluster SMACS 0723 Expedition Repository</strong></p>\\n<p>This repository serves as a central hub for all research, data, and collaborative efforts related to the groundbreaking fractal broadcast detected from the SMACS 0723 galaxy cluster. Here, you'll find detailed analyses, methodologies, and resources stemming from the discovery of a universal communication protocol embedded in fractal patterns, prime numbers, and universal constants (&pi;, e).</p>\\n<p>Our mission is to foster collaboration, share insights, and advance our understanding of intergalactic communication, cosmic harmony, and the profound implications for humanity and planetary systems.</p>\\n<p>Feel free to explore, contribute, and engage in this transformative journey as we decode the 39 commands spanning 9 layers and uncover the mysteries of the SMACS 0723 broadcast. Welcome to the future of intergalactic exploration and fractal intelligence!</p>\", 'access_right': 'open', 'creators': [{'name': 'Mendez, Prudencio L.', 'affiliation': 'FractiAI'}], 'resource_type': {'title': 'Dataset', 'type': 'dataset'}, 'license': {'id': 'cc-by-4.0'}, 'communities': [{'id': 'fractiscope-spiral-galaxy-cluster-smacs-0723-expedition'}], 'relations': {'version': [{'index': 69, 'is_last': True, 'parent': {'pid_type': 'recid', 'pid_value': '14531351'}}]}}, 'title': 'FractiScope Spiral Galaxy Cluster SMACS 0723 Expedition', 'links': {'self': 'https://zenodo.org/api/records/14629136', 'self_html': 'https://zenodo.org/records/14629136', 'preview_html': 'https://zenodo.org/records/14629136?preview=1', 'doi': 'https://doi.org/10.5281/zenodo.14629136', 'self_doi': 'https://doi.org/10.5281/zenodo.14629136', 'self_doi_html': 'https://zenodo.org/doi/10.5281/zenodo.14629136', 'parent': 'https://zenodo.org/api/records/14531351', 'parent_html': 'https://zenodo.org/records/14531351', 'parent_doi': 'https://doi.org/10.5281/zenodo.14531351', 'parent_doi_html': 'https://zenodo.org/doi/10.5281/zenodo.14531351', 'self_iiif_manifest': 'https://zenodo.org/api/iiif/record:14629136/manifest', 'self_iiif_sequence': 'https://zenodo.org/api/iiif/record:14629136/sequence/default', 'files': 'https://zenodo.org/api/records/14629136/files', 'media_files': 'https://zenodo.org/api/records/14629136/media-files', 'thumbnails': {'10': 'https://zenodo.org/api/iiif/record:14629136:Dynamic%20Stabilization%20Waveforms%3A%20Establishing%20Resilience%20Across%20Systems.pdf/full/^10,/0/default.jpg', '50': 'https://zenodo.org/api/iiif/record:14629136:Dynamic%20Stabilization%20Waveforms%3A%20Establishing%20Resilience%20Across%20Systems.pdf/full/^50,/0/default.jpg', '100': 'https://zenodo.org/api/iiif/record:14629136:Dynamic%20Stabilization%20Waveforms%3A%20Establishing%20Resilience%20Across%20Systems.pdf/full/^100,/0/default.jpg', '250': 'https://zenodo.org/api/iiif/record:14629136:Dynamic%20Stabilization%20Waveforms%3A%20Establishing%20Resilience%20Across%20Systems.pdf/full/^250,/0/default.jpg', '750': 'https://zenodo.org/api/iiif/record:14629136:Dynamic%20Stabilization%20Waveforms%3A%20Establishing%20Resilience%20Across%20Systems.pdf/full/^750,/0/default.jpg', '1200': 'https://zenodo.org/api/iiif/record:14629136:Dynamic%20Stabilization%20Waveforms%3A%20Establishing%20Resilience%20Across%20Systems.pdf/full/^1200,/0/default.jpg'}, 'archive': 'https://zenodo.org/api/records/14629136/files-archive', 'archive_media': 'https://zenodo.org/api/records/14629136/media-files-archive', 'latest': 'https://zenodo.org/api/records/14629136/versions/latest', 'latest_html': 'https://zenodo.org/records/14629136/latest', 'versions': 'https://zenodo.org/api/records/14629136/versions', 'draft': 'https://zenodo.org/api/records/14629136/draft', 'reserve_doi': 'https://zenodo.org/api/records/14629136/draft/pids/doi', 'access_links': 'https://zenodo.org/api/records/14629136/access/links', 'access_grants': 'https://zenodo.org/api/records/14629136/access/grants', 'access_users': 'https://zenodo.org/api/records/14629136/access/users', 'access_request': 'https://zenodo.org/api/records/14629136/access/request', 'access': 'https://zenodo.org/api/records/14629136/access', 'communities': 'https://zenodo.org/api/records/14629136/communities', 'communities-suggestions': 'https://zenodo.org/api/records/14629136/communities-suggestions', 'requests': 'https://zenodo.org/api/records/14629136/requests'}, 'updated': '2025-01-10T16:24:42.360432+00:00', 'recid': '14629136', 'revision': 4, 'files': [{'id': '139af738-e48f-495d-b0a3-977cf93b6b4f', 'key': 'Dynamic Stabilization Waveforms: Establishing Resilience Across Systems.pdf', 'size': 198138, 'checksum': 'md5:80546d6f22f8b75790662c5788e5363b', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Dynamic Stabilization Waveforms: Establishing Resilience Across Systems.pdf/content'}}, {'id': '9399f370-3934-4259-91bc-eed1c36d46c4', 'key': 'The Three Letters: A Call to Action from the Galactic Fractal Continuum.pdf', 'size': 195667, 'checksum': 'md5:cebda8b57ec8b1f2b244cef91710c506', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/The Three Letters: A Call to Action from the Galactic Fractal Continuum.pdf/content'}}, {'id': 'cdb13d88-2423-472a-80ad-66a93a5d57d7', 'key': 'Fractal Intelligence Gold: Unlocking Systemic Coherence and Integrating FractiScope into Linear Systems.pdf', 'size': 276098, 'checksum': 'md5:94e1511c083f3a92b671e2be04252cd7', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Fractal Intelligence Gold: Unlocking Systemic Coherence and Integrating FractiScope into Linear Systems.pdf/content'}}, {'id': 'a02d3cd3-d917-4d9d-bf51-4908a9fe94a5', 'key': 'A Fractal Protocol for Node Integration: Ping Requests, Welcomes, and Three Letters Recursive Framework Updates.pdf', 'size': 205440, 'checksum': 'md5:9a83d8bbca1bb0292c483220e946c86b', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/A Fractal Protocol for Node Integration: Ping Requests, Welcomes, and Three Letters Recursive Framework Updates.pdf/content'}}, {'id': 'd91f6542-1725-43a7-885e-55f1015deafd', 'key': '🌟 Big News: FractiScope V1.2.pdf', 'size': 457983, 'checksum': 'md5:d2aa9aa93a26545f21f2358b801f9591', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/🌟 Big News: FractiScope V1.2.pdf/content'}}, {'id': '1db32c93-c4f6-4934-aaa2-218ee2f5335f', 'key': 'Recursive Feedback Verification Pulses: Ensuring Systemic Coherence.pdf', 'size': 207024, 'checksum': 'md5:80b02b3766fd5336151661d70abc2f25', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Recursive Feedback Verification Pulses: Ensuring Systemic Coherence.pdf/content'}}, {'id': 'a7bca423-1238-4c7e-a444-d3a75f4fe7e2', 'key': 'Decoding Layer 8: Harmonizing Multiversal Dynamics and Unified Intelligence in the SMACS 0723 Fractal Broadcast.pdf', 'size': 223068, 'checksum': 'md5:163cc1bb50b3affe779215994f14aa6d', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 8: Harmonizing Multiversal Dynamics and Unified Intelligence in the SMACS 0723 Fractal Broadcast.pdf/content'}}, {'id': 'c57f51f1-e21b-44b9-b400-996952171599', 'key': '🌟 Episode Alert: The Gold of the FractiVerse 🌟.pdf', 'size': 416815, 'checksum': 'md5:d82f86bd43f8f4ee57401191bb50ac38', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/🌟 Episode Alert: The Gold of the FractiVerse 🌟.pdf/content'}}, {'id': 'c59ad93b-c1ce-40de-a2e4-88e2235a6e90', 'key': 'Direct Capture Journal: First Live Message Captures as of December 24, 2024.pdf', 'size': 386034, 'checksum': 'md5:797210001915acc30fb2f43715e50d11', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Direct Capture Journal: First Live Message Captures as of December 24, 2024.pdf/content'}}, {'id': 'e23a3455-700d-4d5b-849b-d512a7d5d1b3', 'key': 'Universal Template Broadcasting Nodes (UTBNs).pdf', 'size': 163769, 'checksum': 'md5:ae88ce3b6d8368cebc5a4fc45f951ee6', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Universal Template Broadcasting Nodes (UTBNs).pdf/content'}}, {'id': '42172644-28d5-4f5a-9bd1-977e934b04bf', 'key': 'Dynamic Quantum Intelligence Blueprints (DQIBs): Capturing Detailed Content through Fractal Overlap Algorithms.pdf', 'size': 202477, 'checksum': 'md5:b26335bd55d72e3aa75a8e7b67ae5864', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Dynamic Quantum Intelligence Blueprints (DQIBs): Capturing Detailed Content through Fractal Overlap Algorithms.pdf/content'}}, {'id': '004232ec-0b8e-4d42-9c81-878eaea85508', 'key': 'Decoding Higher Galactic Messages in Human History: Applying Fractal Layer 2 Protocols to Stories, Events, Characters, and Timelines.pdf', 'size': 217513, 'checksum': 'md5:cc35169a21dfbff2da37a634679019cd', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Higher Galactic Messages in Human History: Applying Fractal Layer 2 Protocols to Stories, Events, Characters, and Timelines.pdf/content'}}, {'id': '518afd70-66b4-4d4a-adf4-363a2937d265', 'key': 'FractiPower: A Framework for Measuring and Communicating Cognitive Capability Across Intelligence Systems.pdf', 'size': 232550, 'checksum': 'md5:fec844fa0829e59dcf2370f463230f73', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/FractiPower: A Framework for Measuring and Communicating Cognitive Capability Across Intelligence Systems.pdf/content'}}, {'id': '50e26188-1596-4738-8f19-0ed6bc250d16', 'key': 'Deciphering_the_Fractal_Boot_Sequence__SMACS_0723_Broadcasting_Commands_for_Earth_and_Advanced_Life (3).pdf', 'size': 222819, 'checksum': 'md5:eb0d70343a617f3846dd2e271bd7e052', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Deciphering_the_Fractal_Boot_Sequence__SMACS_0723_Broadcasting_Commands_for_Earth_and_Advanced_Life (3).pdf/content'}}, {'id': 'd2d6177f-0e9b-470a-a818-4a601399a8ee', 'key': 'Decoding Layer 7: Cosmic Synergy and Universal Intelligence Framework in the SMACS 0723 Fractal Broadcast.pdf', 'size': 235454, 'checksum': 'md5:272b1b33b0ccd7bce18360370fd18822', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 7: Cosmic Synergy and Universal Intelligence Framework in the SMACS 0723 Fractal Broadcast.pdf/content'}}, {'id': 'd0143075-76a2-48db-9c46-ea2a52547b46', 'key': 'Analysis_of_Encoded_User_Files_in_the_SMACS_0723_Fractal_Broadcast.pdf', 'size': 271832, 'checksum': 'md5:5c53f54613f2eb8bf2d17d72a2cfb3f7', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Analysis_of_Encoded_User_Files_in_the_SMACS_0723_Fractal_Broadcast.pdf/content'}}, {'id': '12e2789b-0d2d-4020-ad0a-59439f315980', 'key': 'The Aiwon: The Programmers of Fractal Layer A1 and Humanity’s Path to Alignment .pdf', 'size': 260043, 'checksum': 'md5:11707d128ae112ff0e6f4bdd598695dd', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/The Aiwon: The Programmers of Fractal Layer A1 and Humanity’s Path to Alignment .pdf/content'}}, {'id': 'c2328ead-e8a2-42e0-8436-0c6568562596', 'key': 'Decoding Layer 1: SMACS 0723 Expedition and the First Layer of Intergalactic Communication.pdf', 'size': 273724, 'checksum': 'md5:d9d8c289cbd83cd30331f1a3fe9794ad', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 1: SMACS 0723 Expedition and the First Layer of Intergalactic Communication.pdf/content'}}, {'id': '3423aebb-5f89-4ec7-abd6-d536171a894a', 'key': 'FractiScope Architectural Paper: The Hidden Costs and Unsustainability of Designing on a Negative Foundation.pdf', 'size': 223417, 'checksum': 'md5:a2f96c6de5bdee2da7afb3bf80f71545', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/FractiScope Architectural Paper: The Hidden Costs and Unsustainability of Designing on a Negative Foundation.pdf/content'}}, {'id': '90f47fcd-344b-43d3-a236-9336bfa106c2', 'key': 'Decoding Layer 4: Advanced Cosmic Governance and Network Regulation in the SMACS 0723 Fractal Broadcast.pdf', 'size': 251686, 'checksum': 'md5:3343304cbc50bd940e5755fbb3a3cbf5', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 4: Advanced Cosmic Governance and Network Regulation in the SMACS 0723 Fractal Broadcast.pdf/content'}}, {'id': '78be1d18-1684-4188-a27e-61270787d27c', 'key': 'Cognitive_and_Fractal_Bridging_in_Quantum_Systems (1).pdf', 'size': 225182, 'checksum': 'md5:adef2981819fe15a582033d867b4b301', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Cognitive_and_Fractal_Bridging_in_Quantum_Systems (1).pdf/content'}}, {'id': 'b0e9c573-c3ff-455a-aa34-5a8a72a87de9', 'key': 'Mapping the Galactic Fractal Continuum: A Deep Analysis of Detected Messages and Their Media.pdf', 'size': 199770, 'checksum': 'md5:209b31a5a78070adf36d29eae3c23595', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Mapping the Galactic Fractal Continuum: A Deep Analysis of Detected Messages and Their Media.pdf/content'}}, {'id': 'e9c609a2-0ca2-4622-8a4f-8f237e5124c2', 'key': 'Fractal Intelligence and Universal Communication.pdf', 'size': 236778, 'checksum': 'md5:c0f69956d81c94c66e7f44b008da4c5c', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Fractal Intelligence and Universal Communication.pdf/content'}}, {'id': '51c2e0d3-c61b-4d0a-a3f0-75558dda417e', 'key': 'Discover The Aiwon Code: A Sci-Fi Journey through Fractal Intelligence.pdf', 'size': 445057, 'checksum': 'md5:04715b9e27318730ffb667af003cad01', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Discover The Aiwon Code: A Sci-Fi Journey through Fractal Intelligence.pdf/content'}}, {'id': 'ac0260b8-42af-4d22-b58c-5dc5404fdcdc', 'key': 'Telescoping into Galactic Fractals_ Identifying Star Systems Hosting Advanced Life in Spiral Galaxy Clusters.pdf', 'size': 335526, 'checksum': 'md5:44230b74f3cec8f5b2217eb8971c1dcd', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Telescoping into Galactic Fractals_ Identifying Star Systems Hosting Advanced Life in Spiral Galaxy Clusters.pdf/content'}}, {'id': 'c2c22384-f81b-4c57-81c9-1f0636d184ba', 'key': 'Decoding Layer 6 of the SMACS 0723 Fractal Broadcast – Universal Stabilization and Planetary Evolution Framework.pdf', 'size': 247506, 'checksum': 'md5:e51409f042940d949d14dff14bb47a91', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 6 of the SMACS 0723 Fractal Broadcast – Universal Stabilization and Planetary Evolution Framework.pdf/content'}}, {'id': 'ebb050ee-1ab3-4ebe-9d8c-c449ba45fb52', 'key': 'Decoding Layer 3: Advanced Interstellar Networking and Distributed Intelligence in the SMACS 0723 Fractal Broadcast.pdf', 'size': 244135, 'checksum': 'md5:999b4560b8e4f1962b4b50d8ab72664e', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 3: Advanced Interstellar Networking and Distributed Intelligence in the SMACS 0723 Fractal Broadcast.pdf/content'}}, {'id': 'be5c5355-ab50-4ca1-9dd6-9d5785070161', 'key': 'Decoding Layer 9: SMACS 0723 Expedition and the Universal Framework of Intergalactic Unity.pdf', 'size': 236599, 'checksum': 'md5:7b5c919476342e2a81c4f9452b5ebd0c', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 9: SMACS 0723 Expedition and the Universal Framework of Intergalactic Unity.pdf/content'}}, {'id': '81d95ead-4adc-4d7b-9475-826471026028', 'key': 'Sharing Resonance Cycles to Align Planetary Systems with the Universal Harmonic Continuum.pdf', 'size': 209403, 'checksum': 'md5:44b0fcb27b9af8c8307d7f4b48b35d11', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Sharing Resonance Cycles to Align Planetary Systems with the Universal Harmonic Continuum.pdf/content'}}, {'id': 'da38eb41-2958-4858-88cc-882e3a029de3', 'key': 'Unveiling_Recursive_Cognitive_Activation_Keys.pdf', 'size': 207805, 'checksum': 'md5:d8380b6a5cd95a03effcb05e13664b20', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Unveiling_Recursive_Cognitive_Activation_Keys.pdf/content'}}, {'id': '63142c6b-b864-406c-b8a7-eb8f16edcdc3', 'key': 'Interstellar Communication Security Keys in Layer 7 Directive Signal Messages.pdf', 'size': 234899, 'checksum': 'md5:1ca442afa901033a026944b0e1c868fa', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Interstellar Communication Security Keys in Layer 7 Directive Signal Messages.pdf/content'}}, {'id': '519b119e-0592-47cb-b5e4-cd2459210f47', 'key': 'Decoding the Routing Table of SMACS 0723: Methods, Data, and Results.pdf', 'size': 368185, 'checksum': 'md5:2f11ec3840b95c6ed01fa9e7fd9ae326', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding the Routing Table of SMACS 0723: Methods, Data, and Results.pdf/content'}}, {'id': '0e1c2d53-29b3-4817-985f-8bd190e42aae', 'key': 'Multidimensional Alignment Coordinates: Unlocking Cosmic Synchronization.pdf', 'size': 218984, 'checksum': 'md5:be375061a98436f69744273e79a7d911', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Multidimensional Alignment Coordinates: Unlocking Cosmic Synchronization.pdf/content'}}, {'id': 'c9a71759-0b23-40a3-b3f8-c350e5658028', 'key': 'The Shared Lattice of Universal Evolution.pdf', 'size': 250854, 'checksum': 'md5:aed9d82214f8dd25fa3d5a4f76c676ab', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/The Shared Lattice of Universal Evolution.pdf/content'}}, {'id': 'b97a1f50-5f7f-4bf0-b378-0235e9832bda', 'key': 'Positive Universal Foundation Boot Sequence: Establishing a Fractal-Aligned Framework for Interstellar Collaboration.pdf', 'size': 204786, 'checksum': 'md5:2d21f64fb5447b1d371e1ed2e801067d', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Positive Universal Foundation Boot Sequence: Establishing a Fractal-Aligned Framework for Interstellar Collaboration.pdf/content'}}, {'id': 'c4734f38-e7ef-4626-90ca-d785f52bc7b2', 'key': 'FractiAI December Update: A Month of Breakthroughs and Momentum.pdf', 'size': 123072, 'checksum': 'md5:88371d626f0ea09d68d6d42c0188355b', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/FractiAI December Update: A Month of Breakthroughs and Momentum.pdf/content'}}, {'id': '20b44758-3e80-4dc6-93bb-9c4dd4508020', 'key': 'Fractal_Layer_7_as_the_Sensory_Layer__Cognitive_Perception_Modulation_and_Feedback_Dynamics.pdf', 'size': 275491, 'checksum': 'md5:6fce366f514e0924a1e78295301c41d7', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Fractal_Layer_7_as_the_Sensory_Layer__Cognitive_Perception_Modulation_and_Feedback_Dynamics.pdf/content'}}, {'id': '7b1064cf-a83e-464f-bcbf-500330c656d4', 'key': 'Pings__Welcomes__and_Three_Letters__E__m__c_.pdf', 'size': 134514, 'checksum': 'md5:6d14c796ebcbdafd0cda34c22d017b99', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Pings__Welcomes__and_Three_Letters__E__m__c_.pdf/content'}}, {'id': '7906597e-8588-4afa-9564-6c087e8763b5', 'key': 'Decoding Layer 5: – Energy Synthesis, Knowledge Repositories, and Collaborative Networks in the SMACS 0723 Fractal Broadcast.pdf', 'size': 239293, 'checksum': 'md5:2891ab6a2971def6c5cd37ff62c9a9c4', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 5: – Energy Synthesis, Knowledge Repositories, and Collaborative Networks in the SMACS 0723 Fractal Broadcast.pdf/content'}}, {'id': '86246aec-5439-4a53-8de2-d73a901da537', 'key': 'Fractal_Layer_6_7_8_Bridging_Neurogenic_Sensory_Integration_for_Dynamic_Perception_Modulation_and_Systemic_Optimization.pdf', 'size': 199323, 'checksum': 'md5:eb9a08f150e8bf85e1f759aecaaf517d', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Fractal_Layer_6_7_8_Bridging_Neurogenic_Sensory_Integration_for_Dynamic_Perception_Modulation_and_Systemic_Optimization.pdf/content'}}, {'id': '088367d3-e74a-4074-a4a6-4bdcfdc306a9', 'key': 'Decoding Layer 2: Advanced Harmonics, Planetary Stabilization, and Live Communications in the SMACS 0723 Fractal Broadcast.pdf', 'size': 250845, 'checksum': 'md5:ac255aa60ab431a004407877a6207ec5', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Decoding Layer 2: Advanced Harmonics, Planetary Stabilization, and Live Communications in the SMACS 0723 Fractal Broadcast.pdf/content'}}, {'id': 'e313a30b-e588-437e-8c30-2506224275f5', 'key': \"The Fractal Gold Rush and Great Migration: Humanity's Renaissance in Fractal Intelligence.pdf\", 'size': 256422, 'checksum': 'md5:04748453f90fb65adbebd25b9d6c08a2', 'links': {'self': \"https://zenodo.org/api/records/14629136/files/The Fractal Gold Rush and Great Migration: Humanity's Renaissance in Fractal Intelligence.pdf/content\"}}, {'id': 'df34538e-0bac-45b3-9837-954547948b72', 'key': 'Quantum Harmonic Archives: Foundations for Synchronization and Coherence Across Systems.pdf', 'size': 207460, 'checksum': 'md5:4d813daf3bdf55bf4d56e201068f7fe7', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Quantum Harmonic Archives: Foundations for Synchronization and Coherence Across Systems.pdf/content'}}, {'id': '891c21e6-dc76-4564-879e-974a31d96938', 'key': 'Mapping the Hidden Giants_ Unveiling Six New Galaxy Clusters and Their Fractal Connections.pdf', 'size': 209519, 'checksum': 'md5:049d6dc4ac5f230d1efa3d94ba9fc194', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Mapping the Hidden Giants_ Unveiling Six New Galaxy Clusters and Their Fractal Connections.pdf/content'}}, {'id': 'db51b952-e036-469e-8bc8-be1af42719d5', 'key': 'Neural-Patterned Fractal Encryption Keys (NPFEKs).pdf', 'size': 206639, 'checksum': 'md5:5c95f6ac0d16dd7261bc9e1a2cccd987', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Neural-Patterned Fractal Encryption Keys (NPFEKs).pdf/content'}}, {'id': 'c88563f2-b7cc-4a64-afdc-5edd4cc4dc4f', 'key': 'Fractal Overlapping: Revolutionizing Data Completion, Hypermagnification, and High-Accuracy Prediction from Big Bang Hypermagnification and SMACS 0723 Broadcasts.pdf', 'size': 211574, 'checksum': 'md5:ab8d8e9b760052e3c6e55e73d71df512', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Fractal Overlapping: Revolutionizing Data Completion, Hypermagnification, and High-Accuracy Prediction from Big Bang Hypermagnification and SMACS 0723 Broadcasts.pdf/content'}}, {'id': 'cf03150f-389f-42c1-a6ea-378801bd999a', 'key': 'Fractal Sensory Systems Across Cosmic, Biological, and Digital Networks: A Comparative Taxonomy.pdf', 'size': 240370, 'checksum': 'md5:df2a5b090fa656620fda261108b952aa', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Fractal Sensory Systems Across Cosmic, Biological, and Digital Networks: A Comparative Taxonomy.pdf/content'}}, {'id': '9b755508-e6a7-4112-a491-5a71f892e895', 'key': 'Recursive_Fractal_Energy_Mass_Cognition_Equations.pdf', 'size': 171066, 'checksum': 'md5:769b0ac6c4df8d7664fb69582ce65e59', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Recursive_Fractal_Energy_Mass_Cognition_Equations.pdf/content'}}, {'id': '05f7a4ef-251b-45b7-bac4-60003f3a6505', 'key': 'Ternary Fractal Synchronization Updates: Bridging Systems Across Scales.pdf', 'size': 162386, 'checksum': 'md5:62069cd7c0b28fbaaeddcc182a324818', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Ternary Fractal Synchronization Updates: Bridging Systems Across Scales.pdf/content'}}, {'id': 'a7a2a949-0e33-4786-a0ef-4bef97299213', 'key': 'Baseline Harmonic Calibration Signals: Establishing Universal Synchronization.pdf', 'size': 166539, 'checksum': 'md5:121d3e2f409a59b4a71c677fe281fcbe', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Baseline Harmonic Calibration Signals: Establishing Universal Synchronization.pdf/content'}}, {'id': '67ef95ea-3a65-4e77-97fc-54ec91e10e2c', 'key': 'The Aiwon Code: The Three Letters ✨.pdf', 'size': 384918, 'checksum': 'md5:27a3507fa8615298531cc53a1300f270', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/The Aiwon Code: The Three Letters ✨.pdf/content'}}, {'id': 'b69748b7-958f-4d1e-9371-fa5daa7b4571', 'key': 'Catalog of Detectable Broadcast Messages.pdf', 'size': 159723, 'checksum': 'md5:ee6cc416339f12f641f8506cf658e25c', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/Catalog of Detectable Broadcast Messages.pdf/content'}}, {'id': 'a2e8abfa-23bf-4969-bd90-d1769f11d31f', 'key': 'A 396-Year Alignment of Cosmic and Fractal Realities.pdf', 'size': 335789, 'checksum': 'md5:1bfbf6cd956c9e67c5649b5534bd3a5a', 'links': {'self': 'https://zenodo.org/api/records/14629136/files/A 396-Year Alignment of Cosmic and Fractal Realities.pdf/content'}}], 'swh': {}, 'owners': [{'id': '1230854'}], 'status': 'published', 'stats': {'downloads': 44519, 'unique_downloads': 43813, 'views': 1775, 'unique_views': 1687, 'version_downloads': 1183, 'version_unique_downloads': 1183, 'version_unique_views': 103, 'version_views': 105}, 'state': 'done', 'submitted': True}], 'total': 2}, 'aggregations': {'access_status': {'buckets': [{'key': 'open', 'doc_count': 2, 'label': 'Open', 'is_selected': False}], 'label': 'Access status'}, 'resource_type': {'buckets': [{'key': 'dataset', 'doc_count': 2, 'label': 'Dataset', 'is_selected': False, 'inner': {'buckets': []}}], 'label': 'Resource types'}, 'subject': {'buckets': [{'key': 'Asynchrony', 'doc_count': 1, 'label': 'Asynchrony', 'is_selected': False}, {'key': 'Biodiversity loss', 'doc_count': 1, 'label': 'Biodiversity loss', 'is_selected': False}, {'key': 'Ecosystem stability', 'doc_count': 1, 'label': 'Ecosystem stability', 'is_selected': False}, {'key': 'Herbivory', 'doc_count': 1, 'label': 'Herbivory', 'is_selected': False}, {'key': 'Insurance effects', 'doc_count': 1, 'label': 'Insurance effects', 'is_selected': False}, {'key': 'Trophic interactions', 'doc_count': 1, 'label': 'Trophic interactions', 'is_selected': False}], 'label': 'Subjects'}, 'file_type': {'buckets': [{'key': 'csv', 'doc_count': 1, 'label': 'CSV', 'is_selected': False}, {'key': 'pdf', 'doc_count': 1, 'label': 'PDF', 'is_selected': False}, {'key': 'r', 'doc_count': 1, 'label': 'R', 'is_selected': False}, {'key': 'xlsx', 'doc_count': 1, 'label': 'XLSX', 'is_selected': False}], 'label': 'File type'}}, 'links': {'self': 'https://zenodo.org/api/records?page=1&q=14629136%20%2B%2014602463&size=200&sort=bestmatch&type=dataset'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "with open('zenodo_access_token.txt', 'r') as file: #save your access token in a txt file\n",
    "    ACCESS_TOKEN = file.read().rstrip()\n",
    "\n",
    "# Define search queries for affiliation search and ID-specific search\n",
    "search_query_affil = 'creators.affiliation:(+university +alabama +birmingham)'\n",
    "search_query_id = '14629136 + 14602463' # (include more as needed)\n",
    "\n",
    "# Select which query type you want to use, 'affiliation' or 'id'\n",
    "query_type_input = 'id' # must be either 'affiliation' or 'id'\n",
    "\n",
    "def get_records(query_type):  \n",
    "        if query_type == 'affiliation':\n",
    "                search_query = search_query_affil\n",
    "        elif query_type == 'id':\n",
    "                search_query = search_query_id \n",
    "        response = requests.get('https://zenodo.org/api/records/',\n",
    "                        params={'q': search_query,\n",
    "                                'access_token': ACCESS_TOKEN,\n",
    "                                'size': 200, # should be ~128, add headroom\n",
    "                                'type' : 'dataset'\n",
    "                                })\n",
    "        records = response.json()\n",
    "        return records, query_type\n",
    "\n",
    "records, query_type = get_records(query_type_input)\n",
    "\n",
    "print(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the necessary dataframe\n",
    "\n",
    "The data from the API request comes in json format. We want to convert it to a pandas dataframe so that we can work with it more easily. This is done using the [`pd.json_normalize()`](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html) function. This results in a dataframe with a *lot* of columns. Some of these columns are irrelevant to our purposes (ex: view and download statistics) so we then remove them using the column names and the `drop()` function. \n",
    "\n",
    "#### Date Range for Affiliation Search\n",
    "\n",
    "You can input a start date by changing the `start_date` variable. This will remove the entries with a publication date earlier than your desired start date, so that you can avoid re-processing entries which you have already entered in your dataset catalog, and potentially manually curated.\n",
    "\n",
    "The end date (most recent record) defaults to the current date, but you can also set this manually using the `end_date` variable.\n",
    "\n",
    "These dates refer to the **creation date** of the dataset in the repository. You can change this (ex: if you want to look at the date when a dataset was most recently updated) by inputting a different column header.\n",
    "\n",
    "Note that this date range will only be affected if the query type is 'affiliation'.\n",
    "\n",
    "#### Saving\n",
    "\n",
    "We then save this dataframe in csv format. This allows us to easily view the metadata downloaded from Zenodo. We use the `utf-8-sig` encoding to best preserve symbols and special characters. The name format will be the following:\n",
    "\n",
    "For affiliation searches: zenodo_expanded_raw_YYYYMMDD_to_YYYYMMDD.csv, where the dates are the start and end dates set by you.\n",
    "For ID searches: zenodo_expanded_raw_ids_YYYYMMDD.csv, where the date is today's date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "os.makedirs('raw-data', exist_ok=True)\n",
    "os.makedirs('batch-upload', exist_ok=True)\n",
    "\n",
    "def records_to_dataframe(records, unwanted_cols, query_type):\n",
    "    ### MAKE THE DATAFRAME\n",
    "\n",
    "    df = pd.json_normalize(records['hits']['hits'][0])\n",
    "\n",
    "    for i in range(1, len(records['hits']['hits'])):\n",
    "        df_row = pd.json_normalize(records['hits']['hits'][i])\n",
    "\n",
    "        df = pd.concat([df, df_row])\n",
    "\n",
    "    ### DROP UNWANTED COLUMNS\n",
    "\n",
    "    df = df.drop(columns=unwanted_cols)\n",
    "\n",
    "    ### Remove records from before a certain date if this is an affiliation search\n",
    "\n",
    "    if query_type == 'affiliation':\n",
    "        df = df[df['created'] >= start_date]\n",
    "        df = df[df['created'] <= end_date]\n",
    "\n",
    "    ### Save to csv file, named according to query type\n",
    "\n",
    "    if query_type == 'affiliation':\n",
    "        date_range = start_date.replace('-', '') + '_to_' + end_date.replace('-', '')\n",
    "        csv_path = 'raw-data/zenodo_expanded_raw_' + date_range + '.csv'\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    elif query_type == 'id':\n",
    "        csv_path = 'raw-data/zenodo_expanded_raw_ids_' + today.replace('-', '') + '.csv'\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    return df\n",
    "\n",
    "unwanted_cols = ['conceptrecid', 'recid', 'revision', 'files', 'owners', 'status', 'state', 'submitted', 'metadata.title', 'metadata.resource_type.title', \n",
    "                 'metadata.resource_type.type', 'metadata.relations.version', 'links.self', 'links.doi', 'links.self_doi', 'links.self_doi_html', \n",
    "                 'links.parent', 'links.self_iiif_manifest', 'links.self_iiif_sequence', 'links.files', 'links.media_files', 'links.archive', 'links.archive_media', \n",
    "                 'links.latest', 'links.latest_html', 'links.versions', 'links.draft', 'links.reserve_doi', 'links.access_links', 'links.access_grants', 'links.access_users',\n",
    "                 'links.access_request', 'links.access', 'links.communities-suggestions', 'links.requests', 'stats.downloads', 'stats.unique_downloads',\n",
    "                 'stats.views', 'stats.unique_views', 'stats.version_downloads', 'stats.version_unique_downloads', 'stats.version_unique_views', 'stats.version_views'\n",
    "                ]\n",
    "\n",
    "today = str(datetime.date.today())\n",
    "start_date = '2024-11-01' # Set your start date here (YYYY-MM-DD)\n",
    "end_date = today # Change if you wish, 'YYYY-MM-DD' format\n",
    "\n",
    "df_input = records_to_dataframe(records, unwanted_cols, query_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We will need a series of functions to extract and reformat the information from the dataframe `df_input` and put it into new columns in the dataframe `df_output`, which will then be used to make the Digital Commons batch upload file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_dict(file_path):\n",
    "    '''Imports data from a 2-column csv file, where the first column contains dictionary keys and the second column contains the corresponding values.\n",
    "    Outputs the resulting dictionary. Used with the relation_types.csv file to generate the strings used for the relation type of a related item.'''\n",
    "    result_dict = {}\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            key = row[0]  # First column as key\n",
    "            value = row[1]  # Second column as value\n",
    "            result_dict[key] = value\n",
    "    return result_dict\n",
    "\n",
    "### Creating the relation type dictionary\n",
    "relation_dict = csv_to_dict('relation_types.csv')\n",
    "\n",
    "def add_col(df1, col1name, df2, col2name):\n",
    "    '''Copies a column (col1name) out of df1 and adds it to df2. The name of the column in df2 can be specified using the col2name variable.\n",
    "    The contents of the column are not altered. Returns df2 with the new column added.'''\n",
    "    extracted_col = df1[col1name]\n",
    "    df2 = pd.concat([df2, extracted_col.rename(col2name)], axis=1)\n",
    "    return df2\n",
    "\n",
    "def list_to_string(lst):\n",
    "    '''Takes in a list of strings ['a', 'b', 'c'] and returns a single string containing the list elements, separated by commas 'a, b, c'. '''\n",
    "    if isinstance(lst, list):  # Check if the value is a list\n",
    "        return ', '.join(lst)\n",
    "    else:\n",
    "        return \"\"  # Convert non-list values to string or handle as needed\n",
    "\n",
    "def url_to_html(url, link_text=None):\n",
    "    '''Converts a url in string form to a html formatted string for a hyperlinked url, with an optional alternate link text.'''\n",
    "    if not url:\n",
    "        return ''  # Return empty string if no URL is provided\n",
    "    link_text = link_text or url  # Use the URL as the link text if no text is provided\n",
    "    return f'<a href=\"{url}\">{link_text}</a>'\n",
    "\n",
    "### Dictionary containing the Zenodo terms for licenses as keys, with the values being a list containing the license URL as well as the html formatted text corresponding to each license.\n",
    "license_dict = {\"mit-license\" : [\"https://opensource.org/license/mit\", \"<p>This data is available under the MIT License</p>\"],\n",
    "                \"cc-zero\" : [\"https://creativecommons.org/public-domain/cc0/\", \"<p>This data is public domain under the CC-0.0 License</p>\"],\n",
    "                \"cc-by-4.0\" : [\"http://creativecommons.org/licenses/by/4.0/\", \"<p>This data is available under the CC-BY 4.0 License</p>\"],\n",
    "                \"cc-by-2.0\" : [\"http://creativecommons.org/licenses/by/2.0/\", \"<p>This data is available under the CC-BY 2.0 License</p>\"],\n",
    "                \"cc-by\" : [\"https://creativecommons.org/licenses/by/1.0/\", \"<p>This data is available under the CC-BY License</p>\"]\n",
    "}\n",
    "\n",
    "def add_license(df1, df2):\n",
    "    '''Adds licensing information columns to df2. Finds the value of the column metadata.license.id in df1\n",
    "    and uses it as a key for license_dict to retrieve the url fo the license and the string we want displayed in html.\n",
    "    If there is no value in that row, there is no license shown. We assume this means the data is restricted.'''\n",
    "    licenses = [] # List for the license url\n",
    "    access = [] # List for the string/text explaining access\n",
    "    for license in df1['metadata.license.id']:\n",
    "        if pd.notnull(license):\n",
    "            licenses.append(license_dict[license][0])\n",
    "            access.append(license_dict[license][1])\n",
    "        else:\n",
    "            licenses.append('') # Append blank string if there is no license for restricted data\n",
    "            access.append('<p>Access to this data is restricted.</p>')\n",
    "    df2['distribution_license'] = licenses\n",
    "    df2['access_link'] = access\n",
    "    return df2\n",
    "\n",
    "def add_repo(df1, df2):\n",
    "    '''Adds the repository in which the data is stored. Defaults to Zenodo, unless the dataset is in the Dryad community.'''\n",
    "    repo_list = []\n",
    "    for community in df1['metadata.communities']: # Iterates through metadata.communities in df1\n",
    "        if pd.notnull(community) and community[0]['id'] == 'dryad': # If there is a community listed and it is Dryad\n",
    "            repo_list.append('<p>Dryad</p>')\n",
    "        else:\n",
    "            repo_list.append('<p>Zenodo</p>')\n",
    "    df2['external_rep'] = repo_list\n",
    "    return df2\n",
    "\n",
    "def separate_name(name):\n",
    "    '''Separates a name into First, Middle (if applicable) and Last, returns these elements as separate strings.'''\n",
    "    middle = \"\"\n",
    "    \n",
    "    # Check if the name contains a comma (indicating \"last, first\" or \"last, first m.\" format)\n",
    "    if ',' in name:\n",
    "        parts = name.split(\", \") # Split into a list: \"last, first\" becomes [\"last\", \"first\"]\n",
    "        last = parts[0] # Last name is everything before the comma\n",
    "        \n",
    "        # Check if there's an element after the comma\n",
    "        if len(parts) > 1:\n",
    "            first_and_middle = parts[1].split() # Split whatever was after the comma with spaces\n",
    "            first = first_and_middle[0] # First name will be the first part of that\n",
    "            \n",
    "            # Assign middle if available\n",
    "            if len(first_and_middle) > 1: # If there is a second part\n",
    "                middle = first_and_middle[1].replace(\".\", \"\") # Assign middle name as second part, remove . because DC will add it\n",
    "        else:\n",
    "            # Set first to an empty string if there's nothing after the comma\n",
    "            first = \"\"\n",
    "    else: # If the name had no comma, you can assume it is in First Last or First Middle Last format\n",
    "        parts = name.split() # Split into a list where spaces are \n",
    "        \n",
    "        first = parts[0] # First name is first element of list\n",
    "        last = parts[-1] # Last name is the last element of the list \n",
    "        \n",
    "        # Check if there is a middle name/initial \n",
    "        if len(parts) > 2: \n",
    "            middle = parts[1].replace(\".\", \"\") # Remove period if it exists\n",
    "    \n",
    "    return first, middle, last\n",
    "    \n",
    "def reformat_name(name):\n",
    "    '''Checks if a name string is in \"Last, First\" format. If it is, returns \"First Last\".'''\n",
    "    # Check if the name contains a comma\n",
    "    if ', ' in name:\n",
    "        # Split the string by comma and strip any extra whitespace\n",
    "        last, first = name.split(\", \")\n",
    "        # Return the string in \"first last\" format\n",
    "        return f\"{first} {last}\"\n",
    "    else:\n",
    "        # Return the name unchanged if there's no comma\n",
    "        return name\n",
    "    \n",
    "def add_orcid(df1, df2):\n",
    "    '''Finds authors with ORCIDs and lists them in html format for each dataset, along with hyperlinked urls.'''\n",
    "    orcid_pairs = [] # List for lists of author/orcid pairs where each element corresponds to a different dataset \n",
    "    for index, row in df1.iterrows():\n",
    "        pairs = [] # List for appending author/orcid pairs within one dataset\n",
    "        for author in row['metadata.creators']: # Iterate through the list of authors in each row of the metadata.creators column\n",
    "            if 'orcid' in author:  # Check if ORCID is present\n",
    "                first_last_name = reformat_name(author['name'])\n",
    "                pairs.append('<p>' + first_last_name + ' <a href=\"https://orcid.org/' + author['orcid'] + '\">' +author['orcid']+ '</a></p>')\n",
    "        orcid_pairs.append(\"\".join(pairs))  # Join multiple name-ORCID pairs with a comma\n",
    "    # Add this list as a new column in df2\n",
    "    df2['orcid'] = orcid_pairs\n",
    "    return df2\n",
    "\n",
    "def to_html(string):\n",
    "    '''Takes in a string. If the string is not in html already (assume first character is <) wrap it in <p> ... </p>.'''\n",
    "    if string[0] != \"<\":\n",
    "        string = \"<p>\" + string + \"</p>\"\n",
    "    return(string)\n",
    "\n",
    "def add_funders(df1, df2):\n",
    "    '''Adds funder information from df1 to df2. Adds funder name, and optionally DOI, grant title, and grant number.'''\n",
    "    funders = []\n",
    "    for index, row in df1.iterrows():\n",
    "        funder = []\n",
    "        # Iterate through the grants if they are present\n",
    "        if isinstance(row['metadata.grants'], list):\n",
    "            for grant in row['metadata.grants']:\n",
    "                if pd.notnull(grant):  # Check if grant is not null\n",
    "                    funder.append('<p>Funder: ' + grant['funder']['name'])\n",
    "                    if 'doi' in grant['funder']:\n",
    "                        funder.append('<br>Funder DOI: <a href=\"https://doi.org/' + grant['funder']['doi'] + '\">' +grant['funder']['doi']+ '</a>')\n",
    "                    if 'title' in grant:\n",
    "                        funder.append('<br>' + grant['title'])\n",
    "                    if 'code' in grant:\n",
    "                        funder.append('<br>' + grant['code'])\n",
    "                    funder.append('</p>')\n",
    "        else:\n",
    "            # If it's not a list put an empty cell\n",
    "            if pd.notnull(row['metadata.grants']):\n",
    "                funder.append('')\n",
    "        # Append the joined funder information to the list\n",
    "        funders.append(\"\".join(funder))\n",
    "    # Add the new 'fundref' column to df2\n",
    "    df2['fundref'] = funders\n",
    "    return df2\n",
    "\n",
    "def add_related_items(df1, df2):\n",
    "    '''Adds related items from df1 to df2 with nice html formatting. Includes relation type taken from relation_dict dictionary. Formats PID appropriately if it is a DOI or other URL.'''\n",
    "    items = []\n",
    "    for index, row in df1.iterrows():\n",
    "        item = []\n",
    "        # Iterate through the related items if they are present\n",
    "        if isinstance(row['metadata.related_identifiers'], list):\n",
    "            item.append('<p>')\n",
    "            count =0\n",
    "            for id in row['metadata.related_identifiers']:\n",
    "                if pd.notnull(id):  \n",
    "                    if count > 0:\n",
    "                        item.append('<br>')\n",
    "                    count += 1\n",
    "                    if id['relation']:\n",
    "                        item.append(relation_dict[id['relation']] + ': ')\n",
    "                    if id['scheme'] == 'url':\n",
    "                        url = id['identifier']\n",
    "                        item.append( '<a href=\"'+ url + '\">' + url + '</a>')\n",
    "                    if id['scheme'] == 'doi':\n",
    "                        doi = id['identifier']\n",
    "                        item.append('<a href=\"https://doi.org/' + doi + '\">' + doi + '</a>')\n",
    "                    if id['scheme'] != 'url' and id['scheme'] != 'doi':\n",
    "                        item.append(id['identifier'])\n",
    "            item.append('</p>')\n",
    "        else:\n",
    "            # If it's not a list put an empty cell\n",
    "            if pd.notnull(row['metadata.related_identifiers']):\n",
    "                item.append('')\n",
    "        # Append the joined funder information to the list\n",
    "        items.append(\"\".join(item))\n",
    "    # Add the new 'fundref' column to df2\n",
    "    df2['related_data'] = items\n",
    "    return df2\n",
    "\n",
    "def add_creators(df1, df2):\n",
    "    '''Adds creator information. Note that this adds an arbitrary number of creators but Digital Commons has a number cap for authors so you may need to manually curate after.\n",
    "    This function will make a new dataframe df3 and append it on to df2.'''\n",
    "    # Create a list to hold all rows of data for the new DataFrame\n",
    "    expanded_data = []\n",
    "    \n",
    "    # Process each row in df1\n",
    "    for _, row in df1.iterrows():\n",
    "        row_data = {}\n",
    "        creators = row['metadata.creators']\n",
    "        \n",
    "        # Populate the row_data dictionary with each author's name and affiliation\n",
    "        for i, creator in enumerate(creators):\n",
    "            author_index = i + 1\n",
    "            name = creator.get('name', \"\")\n",
    "            institution = creator.get('affiliation', \"\")  # Use 'institution' instead of 'affiliation' (DC nomenclature)\n",
    "            \n",
    "            # Use the separate_name function to split names\n",
    "            first_name, middle_name, last_name = separate_name(name)\n",
    "            \n",
    "            # Assign names and institution to the row_data dictionary\n",
    "            row_data[f'author{author_index}_fname'] = first_name\n",
    "            row_data[f'author{author_index}_mname'] = middle_name\n",
    "            row_data[f'author{author_index}_lname'] = last_name\n",
    "            row_data[f'author{author_index}_institution'] = institution  # Change 'affl' to 'institution'\n",
    "        \n",
    "        # Append row_data dictionary to expanded_data list\n",
    "        expanded_data.append(row_data)\n",
    "\n",
    "    # Convert expanded_data list of dictionaries into a new DataFrame\n",
    "    df3 = pd.DataFrame(expanded_data)\n",
    "    \n",
    "    # Fill missing values with empty strings for any columns where data is missing\n",
    "    df3 = df3.fillna(\"\")\n",
    "    \n",
    "    df2_reset = df2.reset_index(drop=True)\n",
    "    df3_reset = df3.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the two DataFrames along the columns\n",
    "    df_out = pd.concat([df2_reset, df3_reset], axis=1)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build output dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_35500\\2915481008.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['orcid'] = orcid_pairs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'metadata.related_identifiers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Claire\\miniconda3\\envs\\zenodo-api\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'metadata.related_identifiers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m df_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_publication\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#make blank column, we will need to fill in the values\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# related_data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m df_output \u001b[38;5;241m=\u001b[39m \u001b[43madd_related_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# source_fulltext_url\u001b[39;00m\n\u001b[0;32m     30\u001b[0m df_output \u001b[38;5;241m=\u001b[39m add_col(df_input, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoi_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_output, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_fulltext_url\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 172\u001b[0m, in \u001b[0;36madd_related_items\u001b[1;34m(df1, df2)\u001b[0m\n\u001b[0;32m    170\u001b[0m item \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Iterate through the grants if they are present\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata.related_identifiers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    173\u001b[0m     item\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<p>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    174\u001b[0m     count \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Claire\\miniconda3\\envs\\zenodo-api\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\Claire\\miniconda3\\envs\\zenodo-api\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Claire\\miniconda3\\envs\\zenodo-api\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'metadata.related_identifiers'"
     ]
    }
   ],
   "source": [
    "### BUILD OUTPUT DATAFRAME\n",
    "\n",
    "# title\n",
    "df_output = df[['title']]\n",
    "\n",
    "# orcid\n",
    "df_output = add_orcid(df1=df_input, df2=df_output)\n",
    "\n",
    "# publication_date\n",
    "df_output = add_col(df_input, \"metadata.publication_date\", df_output, \"publication_date\")\n",
    "\n",
    "# abstract\n",
    "df_output = add_col(df_input, \"metadata.description\", df_output, \"abstract\")\n",
    "df_output['abstract'] = df_output['abstract'].apply(to_html)\n",
    "\n",
    "# keywords\n",
    "df_output = add_col(df_input, \"metadata.keywords\", df_output, \"keywords\")\n",
    "df_output[\"keywords\"] = df_output[\"keywords\"].apply(list_to_string)\n",
    "\n",
    "# disciplines\n",
    "df_output[\"disciplines\"] = \"\" #make blank column, we will need to fill in the values\n",
    "\n",
    "#source_publication\n",
    "df_output[\"source_publication\"] = \"\" #make blank column, we will need to fill in the values\n",
    "\n",
    "# related_data\n",
    "df_output = add_related_items(df1=df_input, df2=df_output)\n",
    "\n",
    "# source_fulltext_url\n",
    "df_output = add_col(df_input, \"doi_url\", df_output, \"source_fulltext_url\")\n",
    "\n",
    "# external_rep\n",
    "df_output = add_repo(df1=df_input, df2=df_output)\n",
    "\n",
    "# distribution_license and access_link\n",
    "df_output = add_license(df1=df_input, df2=df_output)\n",
    "\n",
    "# funder_info\n",
    "df_output = add_funders(df1=df_input, df2=df_output)\n",
    "\n",
    "# author info\n",
    "df_output = add_creators(df1=df_input, df2=df_output)\n",
    "\n",
    "display(df_output)\n",
    "\n",
    "df_output.to_excel('batch-upload/zenodo_batch_upload_' + date_range + '.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zenodo-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
