{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenodo Metadata Initial Bulk Extraction\n",
    "\n",
    "### What does this code do?\n",
    "\n",
    "This code searches Zenodo using the Zenodo API for UAB affiliated datasets. It then downloads those metadata records and converts them into a format that can be used for a Digital Commons bulk upload, with appropriate headers. This file will be saved as an .xlsx file and will require manual resaving to a .xls file for the actual upload. This file will also require manual curation before it is ready for upload, with special attention to:\n",
    "\n",
    "- Cleaning up affiliations, ex:\n",
    "  - Unifying names for a single institution: University of Alabama - Birmingham, UNIVERSITY OF ALABAMA AT BIRMINGHAM\n",
    "   &rarr; University of Alabama at Birmingham\n",
    "  - Un-abbreviating institution names: NYU &rarr; New York University \n",
    "  - Removing extraneous location details: California Digital Library, Oakland, United States of America &rarr; California Digital Library\n",
    "- Checking for names and keywords written in all caps\n",
    "- Checking for special characters or accents that are not formatted properly\n",
    "\n",
    "### What datasets are included?\n",
    "\n",
    "We want to include find datasets where at least one author is affiliated with the University if Alabama at Birmingham. Since Zenodo does not use or require ROR IDs, the best way to find these datasets is by searching in the creator affiliation field for the query string \"university AND alabama AND birmingham\". Since [Zenodo hosts copies of Dryad datasets](https://blog.zenodo.org/2020/03/10/dryad-and-zenodo-our-path-ahead/), this search will locate datasets in both Zenodo and Dryad. As of 11/14/2024, there are 128 total datasets, 41 of which are from Dryad. The Dryad datasets belong to the Zenodo community \"dryad\", allowing us to isolate them if needed.\n",
    "\n",
    "### Import the data as a json\n",
    "\n",
    "The code below uses an access token (from Claire Warner - you can replace it with your own) and a search query ` \"creators.affiliation:(+university +alabama +birmingham)\" `. The `size` parameter allows you to choose the number of results returned. We have 128 results, so it is set at 200 to allow for some headroom if new datasets appear.\n",
    "\n",
    "We are left with `records`, the API response in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "ACCESS_TOKEN = '5lRvVDSnCTTXgdFWLuCN7HLAK2UWKjUbJwPCEiWJxirzVT3VfLsAeHnhflmt'\n",
    "search_query = 'creators.affiliation:(+university +alabama +birmingham)'\n",
    "\n",
    "response = requests.get('https://zenodo.org/api/records/',\n",
    "                        params={'q': search_query,\n",
    "                                'access_token': ACCESS_TOKEN,\n",
    "                                'size': 200, # should be 128, add headroom\n",
    "                                'type' : 'dataset'\n",
    "                                })\n",
    "\n",
    "records = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the necessary dataframe\n",
    "\n",
    "The data from the API request comes in json format. We want to convert it to a pandas dataframe so that we can work with it more easily. This is done using the [`pd.json_normalize()`](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html) function. This results in a dataframe with a *lot* of columns. Some of these columns are irrelevant to our purposes (ex: view and download statistics) so we then remove them using the column names and the `drop()` function. \n",
    "\n",
    "We then save this dataframe in csv format. This allows us to easily view the metadata downloaded from Zenodo. We use the `utf-8-sig` encoding to best preserve symbols and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### MAKE THE DATAFRAME\n",
    "\n",
    "df = pd.json_normalize(records['hits']['hits'][0])\n",
    "\n",
    "for i in range(1, len(records['hits']['hits'])):\n",
    "    df_row = pd.json_normalize(records['hits']['hits'][i])\n",
    "    # df_row = df_row.drop(columns = ['stats'])\n",
    "\n",
    "    df = pd.concat([df, df_row])\n",
    "\n",
    "### DROP UNWANTED COLUMNS\n",
    "\n",
    "unwanted_cols = ['conceptrecid', 'recid', 'revision', 'files', 'owners', 'status', 'state', 'submitted', 'metadata.title', 'metadata.resource_type.title', \n",
    "                 'metadata.resource_type.type', 'metadata.relations.version', 'links.self', 'links.doi', 'links.self_doi', 'links.self_doi_html', \n",
    "                 'links.parent', 'links.self_iiif_manifest', 'links.self_iiif_sequence', 'links.files', 'links.media_files', 'links.archive', 'links.archive_media', \n",
    "                 'links.latest', 'links.latest_html', 'links.versions', 'links.draft', 'links.reserve_doi', 'links.access_links', 'links.access_grants', 'links.access_users',\n",
    "                 'links.access_request', 'links.access', 'links.communities-suggestions', 'links.requests', 'stats.downloads', 'stats.unique_downloads',\n",
    "                 'stats.views', 'stats.unique_views', 'stats.version_downloads', 'stats.version_unique_downloads', 'stats.version_unique_views', 'stats.version_views'\n",
    "                ]\n",
    "\n",
    "df = df.drop(columns=unwanted_cols)\n",
    "\n",
    "### Rename and save to csv file\n",
    "\n",
    "df_input = df\n",
    "\n",
    "csv_path = 'raw-data/zenodo_expanded_raw.csv'\n",
    "\n",
    "df_input.to_csv(csv_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We will need a series of functions to extract and reformat the information from the dataframe `df_input` and put it into new columns in the dataframe `df_output`, which will then be used to make the Digital Commons batch upload file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_dict(file_path):\n",
    "    '''Imports data from a 2-column csv file, where the first column contains dictionary keys and the second column contains the corresponding values.\n",
    "    Outputs the resulting dictionary. Used with the relation_types.csv file to generate the strings used for the relation type of a related item.'''\n",
    "    result_dict = {}\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            key = row[0]  # First column as key\n",
    "            value = row[1]  # Second column as value\n",
    "            result_dict[key] = value\n",
    "    return result_dict\n",
    "\n",
    "### Creating the relation type dictionary\n",
    "relation_dict = csv_to_dict('relation_types.csv')\n",
    "\n",
    "def add_col(df1, col1name, df2, col2name):\n",
    "    '''Copies a column (col1name) out of df1 and adds it to df2. The name of the column in df2 can be specified using the col2name variable.\n",
    "    The contents of the column are not altered. Returns df2 with the new column added.'''\n",
    "    extracted_col = df1[col1name]\n",
    "    df2 = pd.concat([df2, extracted_col.rename(col2name)], axis=1)\n",
    "    return df2\n",
    "\n",
    "def list_to_string(lst):\n",
    "    '''Takes in a list of strings ['a', 'b', 'c'] and returns a single string containing the list elements, separated by commas 'a, b, c'. '''\n",
    "    if isinstance(lst, list):  # Check if the value is a list\n",
    "        return ', '.join(lst)\n",
    "    else:\n",
    "        return \"\"  # Convert non-list values to string or handle as needed\n",
    "\n",
    "def url_to_html(url, link_text=None):\n",
    "    '''Converts a url in string form to a html formatted string for a hyperlinked url, with an optional alternate link text.'''\n",
    "    if not url:\n",
    "        return ''  # Return empty string if no URL is provided\n",
    "    link_text = link_text or url  # Use the URL as the link text if no text is provided\n",
    "    return f'<a href=\"{url}\">{link_text}</a>'\n",
    "\n",
    "### Dictionary containing the Zenodo terms for licenses as keys, with the values being a list containing the license URL as well as the html formatted text corresponding to each license.\n",
    "license_dict = {\"mit-license\" : [\"https://opensource.org/license/mit\", \"<p>This data is available under the MIT License</p>\"],\n",
    "                \"cc-zero\" : [\"https://creativecommons.org/public-domain/cc0/\", \"<p>This data is public domain under the CC-0.0 License</p>\"],\n",
    "                \"cc-by-4.0\" : [\"http://creativecommons.org/licenses/by/4.0/\", \"<p>This data is available under the CC-BY 4.0 License</p>\"],\n",
    "                \"cc-by-2.0\" : [\"http://creativecommons.org/licenses/by/2.0/\", \"<p>This data is available under the CC-BY 2.0 License</p>\"],\n",
    "                \"cc-by\" : [\"https://creativecommons.org/licenses/by/1.0/\", \"<p>This data is available under the CC-BY License</p>\"]\n",
    "}\n",
    "\n",
    "def add_license(df1, df2):\n",
    "    '''Adds licensing information columns to df2. Finds the value of the column metadata.license.id in df1\n",
    "    and uses it as a key for license_dict to retrieve the url fo the license and the string we want displayed in html.\n",
    "    If there is no value in that row, there is no license shown. We assume this means the data is restricted.'''\n",
    "    licenses = [] # List for the license url\n",
    "    access = [] # List for the string/text explaining access\n",
    "    for license in df1['metadata.license.id']:\n",
    "        if pd.notnull(license):\n",
    "            licenses.append(license_dict[license][0])\n",
    "            access.append(license_dict[license][1])\n",
    "        else:\n",
    "            licenses.append('') # Append blank string if there is no license for restricted data\n",
    "            access.append('<p>Access to this data is restricted.</p>')\n",
    "    df2['distribution_license'] = licenses\n",
    "    df2['access_link'] = access\n",
    "    return df2\n",
    "\n",
    "def add_repo(df1, df2):\n",
    "    '''Adds the repository in which the data is stored. Defaults to Zenodo, unless the dataset is in the Dryad community.'''\n",
    "    repo_list = []\n",
    "    for community in df1['metadata.communities']: # Iterates through metadata.communities in df1\n",
    "        if pd.notnull(community) and community[0]['id'] == 'dryad': # If there is a community listed and it is Dryad\n",
    "            repo_list.append('<p>Dryad</p>')\n",
    "        else:\n",
    "            repo_list.append('<p>Zenodo</p>')\n",
    "    df2['external_rep'] = repo_list\n",
    "    return df2\n",
    "\n",
    "def separate_name(name):\n",
    "    '''Separates a name into First, Middle (if applicable) and Last, returns these elements as separate strings.'''\n",
    "    middle = \"\"\n",
    "    \n",
    "    # Check if the name contains a comma (indicating \"last, first\" or \"last, first m.\" format)\n",
    "    if ',' in name:\n",
    "        parts = name.split(\", \") # Split into a list: \"last, first\" becomes [\"last\", \"first\"]\n",
    "        last = parts[0] # Last name is everything before the comma\n",
    "        \n",
    "        # Check if there's an element after the comma\n",
    "        if len(parts) > 1:\n",
    "            first_and_middle = parts[1].split() # Split whatever was after the comma with spaces\n",
    "            first = first_and_middle[0] # First name will be the first part of that\n",
    "            \n",
    "            # Assign middle if available\n",
    "            if len(first_and_middle) > 1: # If there is a second part\n",
    "                middle = first_and_middle[1].replace(\".\", \"\") # Assign middle name as second part, remove . because DC will add it\n",
    "        else:\n",
    "            # Set first to an empty string if there's nothing after the comma\n",
    "            first = \"\"\n",
    "    else: # If the name had no comma, you can assume it is in First Last or First Middle Last format\n",
    "        parts = name.split() # Split into a list where spaces are \n",
    "        \n",
    "        first = parts[0] # First name is first element of list\n",
    "        last = parts[-1] # Last name is the last element of the list \n",
    "        \n",
    "        # Check if there is a middle name/initial \n",
    "        if len(parts) > 2: \n",
    "            middle = parts[1].replace(\".\", \"\") # Remove period if it exists\n",
    "    \n",
    "    return first, middle, last\n",
    "    \n",
    "def reformat_name(name):\n",
    "    '''Checks if a name string is in \"Last, First\" format. If it is, returns \"First Last\".'''\n",
    "    # Check if the name contains a comma\n",
    "    if ', ' in name:\n",
    "        # Split the string by comma and strip any extra whitespace\n",
    "        last, first = name.split(\", \")\n",
    "        # Return the string in \"first last\" format\n",
    "        return f\"{first} {last}\"\n",
    "    else:\n",
    "        # Return the name unchanged if there's no comma\n",
    "        return name\n",
    "    \n",
    "def add_orcid(df1, df2):\n",
    "    '''Finds authors with ORCIDs and lists them in html format for each dataset, along with hyperlinked urls.'''\n",
    "    orcid_pairs = [] # List for lists of author/orcid pairs where each element corresponds to a different dataset \n",
    "    for index, row in df1.iterrows():\n",
    "        pairs = [] # List for appending author/orcid pairs within one dataset\n",
    "        for author in row['metadata.creators']: # Iterate through the list of authors in each row of the metadata.creators column\n",
    "            if 'orcid' in author:  # Check if ORCID is present\n",
    "                first_last_name = reformat_name(author['name'])\n",
    "                pairs.append('<p>' + first_last_name + ' <a href=\"https://orcid.org/' + author['orcid'] + '\">' +author['orcid']+ '</a></p>')\n",
    "        orcid_pairs.append(\"\".join(pairs))  # Join multiple name-ORCID pairs with a comma\n",
    "    # Add this list as a new column in df2\n",
    "    df2['orcid'] = orcid_pairs\n",
    "    return df2\n",
    "\n",
    "def to_html(string):\n",
    "    '''Takes in a string. If the string is not in html already (assume first character is <) wrap it in <p> ... </p>.'''\n",
    "    if string[0] != \"<\":\n",
    "        string = \"<p>\" + string + \"</p>\"\n",
    "    return(string)\n",
    "\n",
    "def add_funders(df1, df2):\n",
    "    '''Adds funder information from df1 to df2. Adds funder name, and optionally DOI, grant title, and grant number.'''\n",
    "    funders = []\n",
    "    for index, row in df1.iterrows():\n",
    "        funder = []\n",
    "        # Iterate through the grants if they are present\n",
    "        if isinstance(row['metadata.grants'], list):\n",
    "            for grant in row['metadata.grants']:\n",
    "                if pd.notnull(grant):  # Check if grant is not null\n",
    "                    funder.append('<p>Funder: ' + grant['funder']['name'])\n",
    "                    if 'doi' in grant['funder']:\n",
    "                        funder.append('<br>Funder DOI: <a href=\"https://doi.org/' + grant['funder']['doi'] + '\">' +grant['funder']['doi']+ '</a>')\n",
    "                    if 'title' in grant:\n",
    "                        funder.append('<br>' + grant['title'])\n",
    "                    if 'code' in grant:\n",
    "                        funder.append('<br>' + grant['code'])\n",
    "                    funder.append('</p>')\n",
    "        else:\n",
    "            # If it's not a list put an empty cell\n",
    "            if pd.notnull(row['metadata.grants']):\n",
    "                funder.append('')\n",
    "        # Append the joined funder information to the list\n",
    "        funders.append(\"\".join(funder))\n",
    "    # Add the new 'fundref' column to df2\n",
    "    df2['fundref'] = funders\n",
    "    return df2\n",
    "\n",
    "def add_related_items(df1, df2):\n",
    "    '''Adds related items from df1 to df2 with nice html formatting. Includes relation type taken from relation_dict dictionary. Formats PID appropriately if it is a DOI or other URL.'''\n",
    "    items = []\n",
    "    for index, row in df1.iterrows():\n",
    "        item = []\n",
    "        # Iterate through the grants if they are present\n",
    "        if isinstance(row['metadata.related_identifiers'], list):\n",
    "            item.append('<p>')\n",
    "            count =0\n",
    "            for id in row['metadata.related_identifiers']:\n",
    "                if pd.notnull(id):  \n",
    "                    if count > 0:\n",
    "                        item.append('<br>')\n",
    "                    count += 1\n",
    "                    if id['relation']:\n",
    "                        item.append(relation_dict[id['relation']] + ': ')\n",
    "                    if id['scheme'] == 'url':\n",
    "                        url = id['identifier']\n",
    "                        item.append( '<a href=\"'+ url + '\">' + url + '</a>')\n",
    "                    if id['scheme'] == 'doi':\n",
    "                        doi = id['identifier']\n",
    "                        item.append('<a href=\"https://doi.org/' + doi + '\">' + doi + '</a>')\n",
    "                    if id['scheme'] != 'url' and id['scheme'] != 'doi':\n",
    "                        item.append(id['identifier'])\n",
    "            item.append('</p>')\n",
    "        else:\n",
    "            # If it's not a list put an empty cell\n",
    "            if pd.notnull(row['metadata.related_identifiers']):\n",
    "                item.append('')\n",
    "        # Append the joined funder information to the list\n",
    "        items.append(\"\".join(item))\n",
    "    # Add the new 'fundref' column to df2\n",
    "    df2['related_data'] = items\n",
    "    return df2\n",
    "\n",
    "def add_creators(df1, df2):\n",
    "    '''Adds creator information. Note that this adds an arbitrary number of creators but Digital Commons has a number cap for authors so you may need to manually curate after.\n",
    "    This function will make a new dataframe df3 and append it on to df2.'''\n",
    "    # Create a list to hold all rows of data for the new DataFrame\n",
    "    expanded_data = []\n",
    "    \n",
    "    # Process each row in df1\n",
    "    for _, row in df1.iterrows():\n",
    "        row_data = {}\n",
    "        creators = row['metadata.creators']\n",
    "        \n",
    "        # Populate the row_data dictionary with each author's name and affiliation\n",
    "        for i, creator in enumerate(creators):\n",
    "            author_index = i + 1\n",
    "            name = creator.get('name', \"\")\n",
    "            institution = creator.get('affiliation', \"\")  # Use 'institution' instead of 'affiliation' (DC nomenclature)\n",
    "            \n",
    "            # Use the separate_name function to split names\n",
    "            first_name, middle_name, last_name = separate_name(name)\n",
    "            \n",
    "            # Assign names and institution to the row_data dictionary\n",
    "            row_data[f'author{author_index}_fname'] = first_name\n",
    "            row_data[f'author{author_index}_mname'] = middle_name\n",
    "            row_data[f'author{author_index}_lname'] = last_name\n",
    "            row_data[f'author{author_index}_institution'] = institution  # Change 'affl' to 'institution'\n",
    "        \n",
    "        # Append row_data dictionary to expanded_data list\n",
    "        expanded_data.append(row_data)\n",
    "\n",
    "    # Convert expanded_data list of dictionaries into a new DataFrame\n",
    "    df3 = pd.DataFrame(expanded_data)\n",
    "    \n",
    "    # Fill missing values with empty strings for any columns where data is missing\n",
    "    df3 = df3.fillna(\"\")\n",
    "    \n",
    "    df2_reset = df2.reset_index(drop=True)\n",
    "    df3_reset = df3.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the two DataFrames along the columns\n",
    "    df_out = pd.concat([df2_reset, df3_reset], axis=1)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build output dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_33380\\1966582119.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['orcid'] = orcid_pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>orcid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>disciplines</th>\n",
       "      <th>source_publication</th>\n",
       "      <th>related_data</th>\n",
       "      <th>source_fulltext_url</th>\n",
       "      <th>external_rep</th>\n",
       "      <th>...</th>\n",
       "      <th>author47_lname</th>\n",
       "      <th>author47_institution</th>\n",
       "      <th>author48_fname</th>\n",
       "      <th>author48_mname</th>\n",
       "      <th>author48_lname</th>\n",
       "      <th>author48_institution</th>\n",
       "      <th>author49_fname</th>\n",
       "      <th>author49_mname</th>\n",
       "      <th>author49_lname</th>\n",
       "      <th>author49_institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alzheimer's disease risk gene BIN1 induces Tau...</td>\n",
       "      <td>&lt;p&gt;Yuliya Voskobiynyk &lt;a href=\"https://orcid.o...</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>&lt;p&gt;Genome-wide association studies identified ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is cited by: &lt;a href=\"https://doi.org/10.75...</td>\n",
       "      <td>https://doi.org/10.5061/dryad.rbnzs7h8z</td>\n",
       "      <td>&lt;p&gt;Dryad&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data from: The effect of Speed of Processing t...</td>\n",
       "      <td></td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>&lt;p&gt;Older adults experience cognitive deficits ...</td>\n",
       "      <td>peripheral, Useful Field of View, cognitive in...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is cited by: &lt;a href=\"https://doi.org/10.13...</td>\n",
       "      <td>https://doi.org/10.5061/dryad.4fn70</td>\n",
       "      <td>&lt;p&gt;Dryad&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data for Cell-type-specific alternative splici...</td>\n",
       "      <td>&lt;p&gt;Emma F. Jones &lt;a href=\"https://orcid.org/00...</td>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;&lt;strong&gt;data.tar.gz &lt;/strong&gt;contains...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is compiled by: &lt;a href=\"https://doi.org/10...</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.12535061</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data for Altered Glia-Neuron Communication in ...</td>\n",
       "      <td>&lt;p&gt;Tabea Soelter &lt;a href=\"https://orcid.org/00...</td>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;data.tar.gz contains all files from...</td>\n",
       "      <td>Alzheimer's disease, neurodegeneration, cell-c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is supplemented by: &lt;a href=\"https://doi.or...</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.10214497</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data for Long-read RNA sequencing identifies r...</td>\n",
       "      <td>&lt;p&gt;Emma F. Jones &lt;a href=\"https://orcid.org/00...</td>\n",
       "      <td>2023-12-14</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;data_minus_bam.tar.gz contains all fi...</td>\n",
       "      <td>long-read RNA sequencing, brain, sex, alternat...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Is supplement to: &lt;a href=\"https://github.c...</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.10381745</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Dataset for \"What is Gab? A Bastion of Free Sp...</td>\n",
       "      <td></td>\n",
       "      <td>2018-09-13</td>\n",
       "      <td>&lt;p&gt;This dataset was used for this project: &amp;qu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.3460400</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Public Dataset for \"Large Scale Crowdsourcing ...</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-21</td>\n",
       "      <td>&lt;p&gt;Dataset for the &amp;quot;Large Scale Crowdsour...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.3678559</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Transposon DNA sequences facilitate the tissue...</td>\n",
       "      <td></td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>&lt;p&gt;The uploaded files are in the&amp;nbsp;fasta fo...</td>\n",
       "      <td>Horizontal gene transfer, circulating tumor DN...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.7958520</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Functional connectivity in the face of congeni...</td>\n",
       "      <td>&lt;p&gt;Pinar Demirayak &lt;a href=\"https://orcid.org/...</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>&lt;p&gt;Results of diffusion tensor imaging analysi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.3401600</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>CORTICAL CONNECTIVITY IN THE FACE OF CONGENITA...</td>\n",
       "      <td>&lt;p&gt;Pinar Demirayak &lt;a href=\"https://orcid.org/...</td>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>&lt;p&gt;Results of structural and functional connec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://doi.org/10.5281/zenodo.4598198</td>\n",
       "      <td>&lt;p&gt;Zenodo&lt;/p&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Alzheimer's disease risk gene BIN1 induces Tau...   \n",
       "1    Data from: The effect of Speed of Processing t...   \n",
       "2    Data for Cell-type-specific alternative splici...   \n",
       "3    Data for Altered Glia-Neuron Communication in ...   \n",
       "4    Data for Long-read RNA sequencing identifies r...   \n",
       "..                                                 ...   \n",
       "124  Dataset for \"What is Gab? A Bastion of Free Sp...   \n",
       "125  Public Dataset for \"Large Scale Crowdsourcing ...   \n",
       "126  Transposon DNA sequences facilitate the tissue...   \n",
       "127  Functional connectivity in the face of congeni...   \n",
       "128  CORTICAL CONNECTIVITY IN THE FACE OF CONGENITA...   \n",
       "\n",
       "                                                 orcid publication_date  \\\n",
       "0    <p>Yuliya Voskobiynyk <a href=\"https://orcid.o...       2020-08-19   \n",
       "1                                                            2015-08-04   \n",
       "2    <p>Emma F. Jones <a href=\"https://orcid.org/00...       2024-06-25   \n",
       "3    <p>Tabea Soelter <a href=\"https://orcid.org/00...       2023-11-28   \n",
       "4    <p>Emma F. Jones <a href=\"https://orcid.org/00...       2023-12-14   \n",
       "..                                                 ...              ...   \n",
       "124                                                          2018-09-13   \n",
       "125                                                          2020-02-21   \n",
       "126                                                          2023-05-24   \n",
       "127  <p>Pinar Demirayak <a href=\"https://orcid.org/...       2019-09-06   \n",
       "128  <p>Pinar Demirayak <a href=\"https://orcid.org/...       2021-03-11   \n",
       "\n",
       "                                              abstract  \\\n",
       "0    <p>Genome-wide association studies identified ...   \n",
       "1    <p>Older adults experience cognitive deficits ...   \n",
       "2    <p><span><strong>data.tar.gz </strong>contains...   \n",
       "3    <p><strong>data.tar.gz contains all files from...   \n",
       "4    <p><span>data_minus_bam.tar.gz contains all fi...   \n",
       "..                                                 ...   \n",
       "124  <p>This dataset was used for this project: &qu...   \n",
       "125  <p>Dataset for the &quot;Large Scale Crowdsour...   \n",
       "126  <p>The uploaded files are in the&nbsp;fasta fo...   \n",
       "127  <p>Results of diffusion tensor imaging analysi...   \n",
       "128  <p>Results of structural and functional connec...   \n",
       "\n",
       "                                              keywords disciplines  \\\n",
       "0                                                                    \n",
       "1    peripheral, Useful Field of View, cognitive in...               \n",
       "2                                                                    \n",
       "3    Alzheimer's disease, neurodegeneration, cell-c...               \n",
       "4    long-read RNA sequencing, brain, sex, alternat...               \n",
       "..                                                 ...         ...   \n",
       "124                                                                  \n",
       "125                                                                  \n",
       "126  Horizontal gene transfer, circulating tumor DN...               \n",
       "127                                                                  \n",
       "128                                                                  \n",
       "\n",
       "    source_publication                                       related_data  \\\n",
       "0                       <p>Is cited by: <a href=\"https://doi.org/10.75...   \n",
       "1                       <p>Is cited by: <a href=\"https://doi.org/10.13...   \n",
       "2                       <p>Is compiled by: <a href=\"https://doi.org/10...   \n",
       "3                       <p>Is supplemented by: <a href=\"https://doi.or...   \n",
       "4                       <p>Is supplement to: <a href=\"https://github.c...   \n",
       "..                 ...                                                ...   \n",
       "124                                                                         \n",
       "125                                                                         \n",
       "126                                                                         \n",
       "127                                                                         \n",
       "128                                                                         \n",
       "\n",
       "                         source_fulltext_url   external_rep  ...  \\\n",
       "0    https://doi.org/10.5061/dryad.rbnzs7h8z   <p>Dryad</p>  ...   \n",
       "1        https://doi.org/10.5061/dryad.4fn70   <p>Dryad</p>  ...   \n",
       "2    https://doi.org/10.5281/zenodo.12535061  <p>Zenodo</p>  ...   \n",
       "3    https://doi.org/10.5281/zenodo.10214497  <p>Zenodo</p>  ...   \n",
       "4    https://doi.org/10.5281/zenodo.10381745  <p>Zenodo</p>  ...   \n",
       "..                                       ...            ...  ...   \n",
       "124   https://doi.org/10.5281/zenodo.3460400  <p>Zenodo</p>  ...   \n",
       "125   https://doi.org/10.5281/zenodo.3678559  <p>Zenodo</p>  ...   \n",
       "126   https://doi.org/10.5281/zenodo.7958520  <p>Zenodo</p>  ...   \n",
       "127   https://doi.org/10.5281/zenodo.3401600  <p>Zenodo</p>  ...   \n",
       "128   https://doi.org/10.5281/zenodo.4598198  <p>Zenodo</p>  ...   \n",
       "\n",
       "    author47_lname author47_institution author48_fname author48_mname  \\\n",
       "0                                                                       \n",
       "1                                                                       \n",
       "2                                                                       \n",
       "3                                                                       \n",
       "4                                                                       \n",
       "..             ...                  ...            ...            ...   \n",
       "124                                                                     \n",
       "125                                                                     \n",
       "126                                                                     \n",
       "127                                                                     \n",
       "128                                                                     \n",
       "\n",
       "    author48_lname author48_institution author49_fname author49_mname  \\\n",
       "0                                                                       \n",
       "1                                                                       \n",
       "2                                                                       \n",
       "3                                                                       \n",
       "4                                                                       \n",
       "..             ...                  ...            ...            ...   \n",
       "124                                                                     \n",
       "125                                                                     \n",
       "126                                                                     \n",
       "127                                                                     \n",
       "128                                                                     \n",
       "\n",
       "    author49_lname author49_institution  \n",
       "0                                        \n",
       "1                                        \n",
       "2                                        \n",
       "3                                        \n",
       "4                                        \n",
       "..             ...                  ...  \n",
       "124                                      \n",
       "125                                      \n",
       "126                                      \n",
       "127                                      \n",
       "128                                      \n",
       "\n",
       "[129 rows x 209 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### BUILD OUTPUT DATAFRAME\n",
    "\n",
    "# title\n",
    "df_output = df[['title']]\n",
    "\n",
    "# orcid\n",
    "df_output = add_orcid(df1=df_input, df2=df_output)\n",
    "\n",
    "# publication_date\n",
    "df_output = add_col(df_input, \"metadata.publication_date\", df_output, \"publication_date\")\n",
    "\n",
    "# abstract\n",
    "df_output = add_col(df_input, \"metadata.description\", df_output, \"abstract\")\n",
    "df_output['abstract'] = df_output['abstract'].apply(to_html)\n",
    "\n",
    "# keywords\n",
    "df_output = add_col(df_input, \"metadata.keywords\", df_output, \"keywords\")\n",
    "df_output[\"keywords\"] = df_output[\"keywords\"].apply(list_to_string)\n",
    "\n",
    "# disciplines\n",
    "df_output[\"disciplines\"] = \"\" #make blank column, we will need to fill in the values\n",
    "\n",
    "#source_publication\n",
    "df_output[\"source_publication\"] = \"\" #make blank column, we will need to fill in the values\n",
    "\n",
    "# related_data\n",
    "df_output = add_related_items(df1=df_input, df2=df_output)\n",
    "\n",
    "# source_fulltext_url\n",
    "df_output = add_col(df_input, \"doi_url\", df_output, \"source_fulltext_url\")\n",
    "\n",
    "# external_rep\n",
    "df_output = add_repo(df1=df_input, df2=df_output)\n",
    "\n",
    "# distribution_license and access_link\n",
    "df_output = add_license(df1=df_input, df2=df_output)\n",
    "\n",
    "# funder_info\n",
    "df_output = add_funders(df1=df_input, df2=df_output)\n",
    "\n",
    "# author info\n",
    "df_output = add_creators(df1=df_input, df2=df_output)\n",
    "\n",
    "display(df_output)\n",
    "\n",
    "df_output.to_excel('batch-upload/zenodo_batch_upload.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zenodo-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
